{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/sim_models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "import numpy as np\n",
    "\n",
    "class ParaModel(nn.Module):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        super(ParaModel, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab = vocab\n",
    "        self.gpu = args.gpu\n",
    "\n",
    "        self.cosine = CosineSimilarity()\n",
    "\n",
    "    def compute_mask(self, lengths):\n",
    "\n",
    "        lengths = lengths.cpu()\n",
    "        max_len = torch.max(lengths)\n",
    "        range_row = torch.arange(0, max_len).long()[None, :].expand(lengths.size()[0], max_len)\n",
    "        mask = lengths[:, None].expand_as(range_row)\n",
    "        mask = range_row < mask\n",
    "        mask = mask.float()\n",
    "        if self.gpu >= 0:\n",
    "            mask = mask.cuda()\n",
    "        return mask\n",
    "\n",
    "    def torchify_batch(self, batch):\n",
    "\n",
    "        max_len = 0\n",
    "        for i in batch:\n",
    "            if len(i.embeddings) > max_len:\n",
    "                max_len = len(i.embeddings)\n",
    "\n",
    "        batch_len = len(batch)\n",
    "\n",
    "        np_sents = np.zeros((batch_len, max_len), dtype='int32')\n",
    "        np_lens = np.zeros((batch_len,), dtype='int32')\n",
    "\n",
    "        for i, ex in enumerate(batch):\n",
    "            np_sents[i, :len(ex.embeddings)] = ex.embeddings\n",
    "            np_lens[i] = len(ex.embeddings)\n",
    "\n",
    "        idxs, lengths, masks = torch.from_numpy(np_sents).long(), \\\n",
    "                               torch.from_numpy(np_lens).float().long(), \\\n",
    "                               self.compute_mask(torch.from_numpy(np_lens).long())\n",
    "\n",
    "        if self.gpu >= 0:\n",
    "            idxs = idxs.cuda()\n",
    "            lengths = lengths.cuda()\n",
    "            masks = masks.cuda()\n",
    "    \n",
    "        return idxs, lengths, masks\n",
    "\n",
    "    def scoring_function(self, g_idxs1, g_mask1, g_lengths1, g_idxs2, g_mask2, g_lengths2):\n",
    "\n",
    "        g1 = self.encode(g_idxs1, g_mask1, g_lengths1)\n",
    "        g2 = self.encode(g_idxs2, g_mask2, g_lengths2)\n",
    "        return self.cosine(g1, g2)\n",
    "\n",
    "class WordAveraging(ParaModel):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        super(WordAveraging, self).__init__(args, vocab)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embedding = nn.Embedding(len(self.vocab), self.args.dim)\n",
    "\n",
    "        if args.gpu >= 0:\n",
    "           self.cuda()\n",
    "\n",
    "    def encode(self, idxs, mask, lengths):\n",
    "        word_embs = self.embedding(idxs)\n",
    "        word_embs = word_embs * mask[:, :, None]\n",
    "        g = word_embs.sum(dim=1) / lengths[:, None].float()\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/sim_utils.py\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_wordmap(textfile):\n",
    "    words={}\n",
    "    We = []\n",
    "    f = io.open(textfile, 'r', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    if len(lines[0].split()) == 2:\n",
    "        lines.pop(0)\n",
    "    ct = 0\n",
    "    for (n,i) in enumerate(lines):\n",
    "        word = i.split(' ', 1)[0]\n",
    "        vec = i.split(' ', 1)[1].split(' ')\n",
    "        j = 0\n",
    "        v = []\n",
    "        while j < len(vec):\n",
    "            v.append(float(vec[j]))\n",
    "            j += 1\n",
    "        words[word] = ct\n",
    "        ct += 1\n",
    "        We.append(v)\n",
    "    return words, np.array(We)\n",
    "\n",
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)\n",
    "\n",
    "def max_pool(x, lengths, gpu):\n",
    "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
    "    if gpu >= 0:\n",
    "        out = out.cuda()\n",
    "    for i in range(len(lengths)):\n",
    "        out[i] = torch.max(x[i][0:lengths[i]], 0)[0]\n",
    "    return out\n",
    "\n",
    "def mean_pool(x, lengths, gpu):\n",
    "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
    "    if gpu >= 0:\n",
    "        out = out.cuda()\n",
    "    for i in range(len(lengths)):\n",
    "        out[i] = torch.mean(x[i][0:lengths[i]], 0)\n",
    "    return out\n",
    "\n",
    "def lookup(words, w):\n",
    "    w = w.lower()\n",
    "    if w in words:\n",
    "        return words[w]\n",
    "\n",
    "class Example(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence.strip().lower()\n",
    "        self.embeddings = []\n",
    "        self.representation = None\n",
    "\n",
    "    def populate_embeddings(self, words):\n",
    "        sentence = self.sentence.lower()\n",
    "        arr = sentence.split()\n",
    "        for i in arr:\n",
    "            emb = lookup(words, i)\n",
    "            if emb:\n",
    "                self.embeddings.append(emb)\n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings.append(words['UUUNKKK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/test_sim.py\n",
    "import torch\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "tok = TreebankWordTokenizer()\n",
    "\n",
    "model = torch.load('/home/kkatsy/par3/par3_align/similarity/sim/sim.pt')\n",
    "state_dict = model['state_dict']\n",
    "vocab_words = model['vocab_words']\n",
    "args = model['args']\n",
    "# turn off gpu\n",
    "model = WordAveraging(args, vocab_words)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/kkatsy/par3/par3_align/similarity/sim/sim.sp.30k.model')\n",
    "model.eval()\n",
    "\n",
    "def make_example(sentence, model):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = \" \".join(tok.tokenize(sentence))\n",
    "    sentence = sp.EncodeAsPieces(sentence)\n",
    "    wp1 = Example(\" \".join(sentence))\n",
    "    wp1.populate_embeddings(model.vocab)\n",
    "    return wp1\n",
    "\n",
    "def find_similarity(s1, s2):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        s2 = [make_example(x, model) for x in s2]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        wx2, wl2, wm2 = model.torchify_batch(s2)\n",
    "        BATCH_SIZE = 512\n",
    "        all_scores = []\n",
    "        for i in range(0, len(wx1), BATCH_SIZE):\n",
    "            scores = model.scoring_function(wx1[i:i + BATCH_SIZE], wm1[i:i + BATCH_SIZE], wl1[i:i + BATCH_SIZE],\n",
    "                                            wx2[i:i + BATCH_SIZE], wm2[i:i + BATCH_SIZE], wl2[i:i + BATCH_SIZE])\n",
    "            all_scores.extend([x.item() for x in scores])\n",
    "        return all_scores\n",
    "\n",
    "def find_similarity_matrix(s1, s2):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        s2 = [make_example(x, model) for x in s2]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        wx2, wl2, wm2 = model.torchify_batch(s2)\n",
    "\n",
    "        BATCH_SIZE = 2000\n",
    "        vecs1 = []\n",
    "        vecs2 = []\n",
    "        for i in range(0, len(wx1), BATCH_SIZE):\n",
    "            curr_vecs1 = model.encode(idxs=wx1[i:i + BATCH_SIZE],\n",
    "                                      mask=wm1[i:i + BATCH_SIZE],\n",
    "                                      lengths=wl1[i:i + BATCH_SIZE])\n",
    "            vecs1.append(curr_vecs1)\n",
    "        for i in range(0, len(wx2), BATCH_SIZE):\n",
    "            curr_vecs2 = model.encode(idxs=wx2[i:i + BATCH_SIZE],\n",
    "                                      mask=wm2[i:i + BATCH_SIZE],\n",
    "                                      lengths=wl2[i:i + BATCH_SIZE])\n",
    "            vecs2.append(curr_vecs2)\n",
    "        vecs1 = torch.cat(vecs1)\n",
    "        vecs2 = torch.cat(vecs2)\n",
    "        dot_product = torch.matmul(vecs1, vecs2.t())\n",
    "\n",
    "        vecs1_norm = torch.norm(vecs1, dim=1, keepdim=True)\n",
    "        vecs2_norm = torch.norm(vecs2, dim=1, keepdim=True)\n",
    "        norm_product = torch.matmul(vecs1_norm, vecs2_norm.t())\n",
    "    return torch.div(dot_product, norm_product)\n",
    "\n",
    "def encode_text(s1):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        vecs1 = model.encode(idxs=wx1, mask=wm1, lengths=wl1)\n",
    "        return vecs1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordAveraging(\n",
       "  (cosine): CosineSimilarity()\n",
       "  (embedding): Embedding(65733, 300)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = TreebankWordTokenizer()\n",
    "\n",
    "model = torch.load('/home/kkatsy/par3/par3_align/similarity/sim/sim.pt')\n",
    "state_dict = model['state_dict']\n",
    "vocab_words = model['vocab_words']\n",
    "args = model['args']\n",
    "# turn off gpu\n",
    "model = WordAveraging(args, vocab_words)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/kkatsy/par3/par3_align/similarity/sim/sim.sp.30k.model')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(refs, cands, metric='sim'):\n",
    "    return find_similarity(refs,cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('aligned_paragraph_dataset.pickle', 'rb') as fp:\n",
    "  aligned_paragraph_dataset = pickle.load(fp)\n",
    "\n",
    "with open('source_paragraph_dataset.pickle', 'rb') as fp:\n",
    "  source_paragraph_dataset = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "\n",
    "def get_best_alignments(par_list, source_par_list, top_k_percent, num_k, drop_top, metric, min_len, max_len, align_scale):\n",
    "\n",
    "    # dict -> score:par_set\n",
    "    # iter thru par_list, prune by length, get metric for set\n",
    "    keep_index_list = []\n",
    "    i2score = {}\n",
    "    for i in range(len(par_list)):\n",
    "        keep_index_list.append(i)\n",
    "        par_set = par_list[i]\n",
    "\n",
    "        max_par_len = len(max(par_set, key = len))\n",
    "        min_par_len = len(min(par_set, key = len))\n",
    "        source_len = len(source_par_list[i])\n",
    "\n",
    "        if (min_par_len >= min_len) and (max_par_len) <= max_len and not all(x==par_set[0] for x in par_set) and (max_par_len <= align_scale*source_len) and (min_par_len*align_scale >= source_len):\n",
    "\n",
    "            pairs = list(itertools.combinations(par_set, 2))\n",
    "            refs, cands = [], []\n",
    "            for s1, s2 in pairs:\n",
    "                refs.append(s1)\n",
    "                cands.append(s2)\n",
    "                \n",
    "            pair_scores = get_score(refs, cands, metric)\n",
    "\n",
    "            average_score = mean(pair_scores)\n",
    "            i2score[i] = average_score\n",
    "\n",
    "    # get top k par sets\n",
    "    num_pars = len(list(i2score))\n",
    "    top_k = int(top_k_percent * num_pars)\n",
    "    if top_k >= num_k:\n",
    "        top_k_scores = sorted(i2score.items(), key=itemgetter(1), reverse=True)[int(num_pars*drop_top):int(num_pars*drop_top) + num_k]\n",
    "    else:\n",
    "        top_k_scores = sorted(i2score.items(), key=itemgetter(1), reverse=True)[int(num_pars*drop_top):int(num_pars*drop_top) + top_k]\n",
    "    \n",
    "    i2score = sorted(i2score.items(), key=itemgetter(1), reverse=True)\n",
    "    return i2score, top_k_scores, keep_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_paragraph_len = 20\n",
    "max_paragraph_len = 1000000000000\n",
    "top_k_percent = 0.9\n",
    "num_k = 50000\n",
    "drop_top = 0.02\n",
    "align_scale = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout\n",
    "# NotesFromUnderground - Katz, PV, Garnett, Hogarth\n",
    "# PoorFolk - McDuff, Hogarth, Garnett\n",
    "# TheIdiot - Garnett, McDuff, PV\n",
    "# CrimeAndPunishment - Katz, McDuff, PV, Garnett\n",
    "holdout_books = ['TheIdiot', 'NotesFromUnderground']\n",
    "ignore_books = []\n",
    "translator_to_pars = {}\n",
    "translator_to_pars_holdout = {}\n",
    "\n",
    "# for each book in train:\n",
    "for book in sorted(list(aligned_paragraph_dataset.keys())):\n",
    "    # get par list of aligned sentences, best k alignments\n",
    "    book_par_list = [list(aligned_paragraph_dataset[book][p].values()) for p in range(len(aligned_paragraph_dataset[book]))]\n",
    "    source_par_list = source_paragraph_dataset[book]\n",
    "\n",
    "    if book in holdout_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, 1.0, 5000, 0, 'sim', min_paragraph_len, max_paragraph_len, 100)\n",
    "    elif book not in ignore_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, top_k_percent, num_k, drop_top, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    else:\n",
    "        top_k = []\n",
    "\n",
    "    for i, sim in top_k:\n",
    "        par_trans_dict = aligned_paragraph_dataset[book][i]\n",
    "        par_source = source_paragraph_dataset[book][i]\n",
    "\n",
    "        for translator, t in par_trans_dict.items():\n",
    "            t = t.replace('\\\\\\'', '\\'')\n",
    "            datum_dict = {'source':par_source, 'translation': t, 'idx': i, 'book': book, 'sim': sim, 'translator': translator}\n",
    "\n",
    "            if translator not in translator_to_pars.keys():\n",
    "                translator_to_pars[translator] = []\n",
    "                translator_to_pars_holdout[translator] = []\n",
    "                \n",
    "            if book in holdout_books:\n",
    "                translator_to_pars_holdout[translator].append(datum_dict)\n",
    "            else:\n",
    "                translator_to_pars[translator].append(datum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547\n"
     ]
    }
   ],
   "source": [
    "min_len = len(translator_to_pars['Hogarth'])\n",
    "print(min_len)\n",
    "for t in translator_to_pars.keys():\n",
    "    keep = sorted(translator_to_pars[t], key=lambda d: d['sim'], reverse=True)[:min_len]\n",
    "    translator_to_pars[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "min_len_h = len(translator_to_pars_holdout['Hogarth'])\n",
    "print(min_len_h)\n",
    "for t in translator_to_pars_holdout.keys():\n",
    "    keep = sample(translator_to_pars_holdout[t], min_len_h) \n",
    "    translator_to_pars_holdout[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'Из лицея молодой человек в первые два года приезжал на вакацию. Во время поездки в Петербург Варвары Петровны и Степана Трофимовича он присутствовал иногда на литературных вечерах, бывавших у мамаши, слушал и наблюдал. Говорил мало и всё по-прежнему был тих и застенчив. К Степану Трофимовичу относился с прежним нежным вниманием, но уже как-то сдержаннее: о высоких предметах и о воспоминаниях прошлого видимо удалялся с ним заговаривать. Кончив курс, он, по желанию мамаши, поступил в военную службу и вскоре был зачислен в один из самых видных гвардейских кавалерийских полков. Показаться мамаше в мундире он не приехал и редко стал писать из Петербурга. Денег Варвара Петровна посылала ему не жалея, несмотря на то что после реформы доход с ее имений упал до того, что в первое время она и половины прежнего дохода не получала. У ней, впрочем, накоплен был долгою экономией некоторый, не совсем маленький капитал. Ее очень интересовали успехи сына в высшем петербургском обществе. Что не удалось ей, то удалось молодому офицеру, богатому и с надеждами. Он возобновил такие знакомства, о которых она и мечтать уже не могла, и везде был принят с большим удовольствием. Но очень скоро начали доходить к Варваре Петровне довольно странные слухи: молодой человек как-то безумно и вдруг закутил. Не то чтоб он играл или очень пил; рассказывали только о какой-то дикой разнузданности, о задавленных рысаками людях, о зверском поступке с одною дамой хорошего общества, с которою он был в связи, а потом оскорбил ее публично. Что-то даже слишком уж откровенно грязное было в этом деле. Прибавляли сверх того, что он какой-то бретер, привязывается и оскорбляет из удовольствия оскорбить. Варвара Петровна волновалась и тосковала. Степан Трофимович уверял ее, что это только первые, буйные порывы слишком богатой организации, что море уляжется и что всё это похоже на юность принца Гарри, кутившего с Фальстафом, Пойнсом и мистрис Квикли, описанную у Шекспира. Варвара Петровна на этот раз не крикнула: «Вздор, вздор!», как повадилась в последнее время покрикивать очень часто на Степана Трофимовича, а, напротив, очень прислушалась, велела растолковать себе подробнее, сама взяла Шекспира и с чрезвычайным вниманием прочла бессмертную хронику. Но хроника ее не успокоила, да и сходства она не так много нашла. Она лихорадочно ждала ответов на несколько своих писем. Ответы не замедлили; скоро было получено роковое известие, что принц Гарри имел почти разом две дуэли, кругом был виноват в обеих, убил одного из своих противников наповал, а другого искалечил и вследствие таковых деяний был отдан под суд. Дело кончилось разжалованием в солдаты, с лишением прав и ссылкой на службу в один из пехотных армейских полков, да и то еще по особенной милости.',\n",
       "  'translation': 'For the first two years the young man came home from the lycée for vacations. While Varvara Petrovna and Stepan Trofimovich were in Petersburg, he was sometimes present at his mother\\'s literary evenings, listening and observing. He spoke little, and was quiet and shy as before. He treated Stepan Trofimovich with the former tender attentiveness, but now somehow more reservedly: he obviously refrained from talking with him about lofty subjects or memories of the past. In accordance with his mama\\'s wish, after completing his studies he entered military service and was soon enrolled in one of the most distinguished regiments of the Horse Guard. He did not come to show himself to his mama in his uniform and now rarely wrote from Petersburg. Varvara Petrovna sent him money without stint, in spite of the fact that the income from her estates fell so much after the reform that at first she did not get even half of her former income. However, through long economy she had saved up a certain not exactly small sum. She was very interested in her son\\'s successes in Petersburg high society. The young officer, rich and with expectations, succeeded where she had not. He renewed acquaintances of which she could no longer even dream, and was received everywhere with great pleasure. But very soon rather strange rumors began to reach Varvara Petrovna: the young man, somehow madly and suddenly, started leading a wild life. Not that he gambled or drank too much; there was only talk of some savage unbridledness, of some people being run over by horses, of some beastly behavior towards a lady of good society with whom he had had a liaison and whom he afterwards publicly insulted. There was something even too frankly dirty about this affair. It was added, furthermore, that he was some sort of swashbuckler, that he picked on people and insulted them for the pleasure of it. Varvara Petrovna was worried and anguished. Stepan Trofimovich assured her that these were merely the first stormy impulses of an overabundant constitution, that the sea would grow calm, and that it all resembled Shakespeare\\'s description of the youth of Prince Harry, carousing with Falstaff, Poins, and Mistress Quickly. This time Varvara Petrovna did not shout \"Nonsense, nonsense!\" as it had lately become her habit to shout quite often at Stepan Trofimovich, but, on the contrary, paid great heed to him, asked him to explain in more detail, herself took Shakespeare and read the immortal chronicle with extreme attention. But the chronicle did not calm her down, nor did  she find all that much resemblance. She waited feverishly for answers to certain of her letters. The answers were not slow in coming; soon the fatal news was  received that Prince Harry had almost simultaneously fought two duels, was entirely to blame for both of them, had killed one of his opponents on the spot and crippled the other, and as a consequence of such deeds had been brought to trial. The affair ended with his being broken to the ranks, stripped of his rights, and exiled to service in one of the infantry regiments, and even that only by special favor.',\n",
       "  'idx': 91,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9765744209289551,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Вещь короткая; даже, если хотите, по-настоящему это и не анекдот, – посыпался бисер. – Впрочем, романист от безделья мог бы испечь роман. Довольно интересная вещица, Прасковья Ивановна, и я уверен, что Лизавета Николаевна с любопытством выслушает, потому что тут много если не чудных, то причудливых вещей. Лет пять тому, в Петербурге, Николай Всеволодович узнал этого господина, – вот этого самого господина Лебядкина, который стоит разиня рот и, кажется, собирался сейчас улизнуть. Извините, Варвара Петровна. Я вам, впрочем, не советую улепетывать, господин отставной чиновник бывшего провиантского ведомства (видите, я отлично вас помню). И мне и Николаю Всеволодовичу слишком известны ваши здешние проделки, в которых, не забудьте это, вы должны будете дать отчет. Еще раз прошу извинения, Варвара Петровна. Николай Всеволодович называл тогда этого господина своим Фальстафом; это, должно быть (пояснил он вдруг), какой-нибудь бывший характер, burlesque,[105] над которым все смеются и который сам позволяет над собою всем смеяться, лишь бы платили деньги. Николай Всеволодович вел тогда в Петербурге жизнь, так сказать, насмешливую, – другим словом не могу определить ее, потому что в разочарование этот человек не впадет, а делом он и сам тогда пренебрегал заниматься. Я говорю про одно лишь тогдашнее время, Варвара Петровна. У Лебядкина этого была сестра, – вот эта самая, что сейчас здесь сидела. Братец и сестрица не имели своего угла и скитались по чужим. Он бродил под арками Гостиного двора, непременно в бывшем мундире, и останавливал прохожих с виду почище, а что наберет – пропивал. Сестрица же кормилась как птица небесная. Она там в углах помогала и за нужду прислуживала. Содом был ужаснейший; я миную картину этой угловой жизни, – жизни, которой из чудачества предавался тогда и Николай Всеволодович. Я только про тогдашнее время, Варвара Петровна; а что касается до «чудачества», то это его собственное выражение. Он многое от меня не скрывает. Mademoiselle Лебядкина, которой одно время слишком часто пришлось встречать Николая Всеволодовича, была поражена его наружностью. Это был, так сказать, бриллиант на грязном фоне ее жизни. Я плохой описатель чувств, а потому пройду мимо; но ее тотчас же подняли дрянные людишки на смех, и она загрустила. Там вообще над нею смеялись, но прежде она вовсе не замечала того. Голова ее уже и тогда была не в порядке, но тогда все-таки не так, как теперь. Есть основание предположить, что в детстве, через какую-то благодетельницу, она чуть было не получила воспитания. Николай Всеволодович никогда не обращал на нее ни малейшего внимания и играл больше в старые замасленные карты по четверть копейки в преферанс с чиновниками. Но раз, когда ее обижали, он (не спрашивая причины) схватил одного чиновника за шиворот и спустил изо второго этажа в окно. Никаких рыцарских негодований в пользу оскорбленной невинности тут не было; вся операция произошла при общем смехе, и смеялся всех больше Николай Всеволодович сам; когда же всё кончилось благополучно, то помирились и стали пить пунш. Но угнетенная невинность сама про то не забыла. Разумеется, кончилось окончательным сотрясением ее умственных способностей. Повторяю, я плохой описатель чувств, но тут главное мечта. А Николай Всеволодович, как нарочно, еще более раздражал мечту: вместо того чтобы рассмеяться, он вдруг стал обращаться к mademoiselle Лебядкиной с неожиданным уважением. Кириллов, тут бывший (чрезвычайный оригинал, Варвара Петровна, и чрезвычайно отрывистый человек; вы, может быть, когда-нибудь его увидите, он теперь здесь), ну так вот, этот Кириллов, который, по обыкновению, всё молчит, а тут вдруг разгорячился, заметил, я помню, Николаю Всеволодовичу, что тот третирует эту госпожу как маркизу и тем окончательно ее добивает. Прибавлю, что Николай Всеволодович несколько уважал этого Кириллова. Что же, вы думаете, он ему ответил: «Вы полагаете, господин Кириллов, что я смеюсь над нею; разуверьтесь, я в самом деле ее уважаю, потому что она всех нас лучше». И, знаете, таким серьезным тоном сказал. Между тем в эти два-три месяца он, кроме «здравствуйте» да «прощайте», в сущности, не проговорил с ней ни слова. Я, тут бывший, наверно помню, что она до того уже, наконец, дошла, что считала его чем-то вроде жениха своего, не смеющего ее «похитить» единственно потому, что у него много врагов и семейных препятствий или что-то в этом роде. Много тут было смеху! Кончилось тем, что когда Николаю Всеволодовичу пришлось тогда отправляться сюда, он, уезжая, распорядился о ее содержании и, кажется, довольно значительном ежегодном пенсионе, рублей в триста по крайней мере, если не более. Одним словом, положим, всё это с его стороны баловство, фантазия преждевременно уставшего человека, – пусть даже, наконец, как говорил Кириллов, это был новый этюд пресыщенного человека с целью узнать, до чего можно довести сумасшедшую калеку. «Вы, говорит, нарочно выбрали самое последнее существо, калеку, покрытую вечным позором и побоями, – и вдобавок зная, что это существо умирает к вам от комической любви своей, – и вдруг вы нарочно принимаетесь ее морочить, единственно для того, чтобы посмотреть, что из этого выйдет!» Чем, наконец, так особенно виноват человек в фантазиях сумасшедшей женщины, с которой, заметьте, он вряд ли две фразы во всё время выговорил! Есть вещи, Варвара Петровна, о которых не только нельзя умно говорить, но о которых и начинать-то говорить неумно. Ну пусть, наконец, чудачество – но ведь более-то уж ничего нельзя сказать; а между тем теперь вот из этого сделали историю… Мне отчасти известно, Варвара Петровна, о том, что здесь происходит. Рассказчик вдруг оборвал и повернулся было к Лебядкину, но Варвара Петровна остановила его; она была в сильнейшей экзальтации.',\n",
       "  'translation': '\"It\\'s a short matter; in fact, if you like, it\\'s not even an anecdote,\" the beads began spilling out. \"However, a novelist might cook up a novel from it in an idle moment. It\\'s quite an interesting little matter, Praskovya Ivanovna, and I\\'m sure Lizaveta Nikolaevna will listen with curiosity, because there are many things here which, if not queer, are at least quaint. About five years ago, in Petersburg, Nikolai Vsevolodovich got to know this gentleman-this same Mr. Lebyadkin who is standing here with his mouth hanging open and, it seems, was just about to slip away. Forgive me, Varvara Petrovna. Incidentally, I\\'d advise you not to take to your heels, mister retired official of the former supply department (you see, I remember you perfectly). Both I and Nikolai Vsevolodovich are all too well informed of  your local tricks, of which, don\\'t forget, you will have to give an accounting. Once again I ask your forgiveness, Varvara Petrovna. Nikolai Vsevolodovich used to call  this gentleman his Falstaff -that must be some former character,\" he suddenly explained, \"some burlesque everyone laughs at and who allows everyone to laugh at him, so long as they pay money. The life Nikolai Vsevolodovich then led in Petersburg was, so to speak, a jeering one-I cannot define it by any other word, because he was not a man to fall into disillusionment, and he scorned then to do anything serious. I\\'m talking only about that time, Varvara Petrovna. This Lebyadkin had a sister-the very one who was just sitting here. This nice brother and sister had no corner of their own, and wandered about staying with various people. He loitered under the arcades of the Gostiny Dvor, unfailingly wearing his former uniform, and stopped the cleanerlooking passersby, and whatever he collected he would spend on drink. His sister lived like the birds of the air. She helped out in those corners and served in exchange for necessities. It was a most terrible Sodom; I\\'ll pass over the picture of this corner life-the life to which Nikolai Vsevolodovich then gave himself out of whimsicality. This was only then, Varvara Petrovna; and as for \\'whimsicality,\\' the expression is his. There is much that he does not conceal from me. Mademoiselle Lebyadkin, who at a certain period happened to run into Nikolai Vsevolodovich all too often, was struck by his appearance. He was, so to speak, a diamond set against the dirty background of her life. I\\'m a poor describer of feelings, so I\\'ll pass that over; but rotten little people immediately made fun of her, and she grew sad. They generally laughed at her there, but before she didn\\'t notice it. She was already not right in the head then, but less so than now. There\\'s reason to think that in childhood, through some benefactress, she almost received an education. Nikolai Vsevolodovich never paid the slightest attention to her, and rather spent his time playing old greasy cards, the game of preference for quarterkopeck stakes, with some clerks. But once when she was being mistreated, he, without asking why, grabbed one clerk by the scruff of the neck and chucked him out the secondstory window. There wasn\\'t any chivalrous indignation in favor of offended innocence in it; the whole operation took place amid general laughter, and Nikolai Vsevolodovich himself laughed most of all; everything eventually came to a good end, they made peace and began drinking punch. But oppressed innocence herself did not forget it. Of course, it ended with the final shaking of her mental faculties. I repeat, I\\'m a poor describer of feelings, but the main thing here was the dream. And Nikolai Vsevolodovich, as if on purpose, aroused the dream even more; instead of just laughing at it, he suddenly began addressing Mademoiselle Lebyadkin with unexpected esteem. Kirillov, who was there (an exceedingly original man, Varvara Petrovna, and an exceedingly abrupt one; perhaps you\\'ll meet him one day, he\\'s here now), well, so this Kirillov, who ordinarily is always silent, but then suddenly got excited, observed to Nikolai Vsevolodovich, as I remember, that his treating this lady as a marquise was finally going to finish her off. I will add that Nikolai Vsevolodovich had a certain respect for this Kirillov. And how do you think he answered him? \\'You assume, Mr. Kirillov, that I am laughing at her; let me assure you that I do indeed respect her, because she is better than any of us.\\' And, you know, he said it in such a serious tone. Though, in fact, during those two or three months he hadn\\'t said a word to her except \\'hello\\' and \\'goodbye.\\' I, who was there, remember for a certainty that she finally reached the point of regarding him as something like her fiancé, who did not dare to \\'abduct\\' her solely because he had many enemies and family obstacles, or something of the sort. There was much laughter over that! In the end, when Nikolai Vsevolodovich had to come here that time, as he was leaving he arranged for her keep, and it seems it was quite a substantial yearly pension, at least three hundred roubles, if not more. In short, let\\'s say it was all selfindulgence, the fancy of a prematurely weary man-let it be, finally, as Kirillov was saying, a new étude by ajaded man, with the object of finding out what a mad cripple can be brought to. \\'You chose on purpose,\\' he said, \\'the very least of beings, a cripple covered in eternal shame and beatings-and knowing, besides, that this being is dying of her comical love for you-and you suddenly start to flummox her on purpose, solely to see what will come of it!\\' Why, finally, is a man so especially to blame for the fantasy of a mad woman to whom, notice, he had hardly spoken two sentences during that whole time! There are things, Varvara Petrovna, of which it is not only impossible to speak intelligently, but of which it is not intelligent even to begin speaking. Well, let it be whimsicality, finally- but that\\'s all one can say; and yet quite a story has been made of it now... I\\'m partly informed, Varvara Petrovna, of what is going on here.\" The narrator suddenly broke off and was turning to Lebyadkin, but Varvara Petrovna stopped him; she was in the greatest exaltation.',\n",
       "  'idx': 1263,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9765353202819824,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Да кто? Кто велел вам сюда приходить? – допрашивала Варвара Петровна.',\n",
       "  'translation': '\"But, who? Who told you to come here?\" Varvara Petrovna questioned.',\n",
       "  'idx': 1228,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9762787818908691,\n",
       "  'translator': 'PV'},\n",
       " {'source': 'Так называемое у нас имение Степана Трофимовича (душ пятьдесят по старинному счету, и смежное со Скворешниками) было вовсе не его, а принадлежало первой его супруге, а стало быть, теперь их сыну, Петру Степановичу Верховенскому. Степан Трофимович только опекунствовал, а потому, когда птенец оперился, действовал по формальной от него доверенности на управление имением. Сделка для молодого человека была выгодная: он получал с отца в год до тысячи рублей в виде дохода с имения, тогда как оно при новых порядках не давало и пятисот (а может быть, и того менее). Бог знает как установились подобные отношения. Впрочем, всю эту тысячу целиком высылала Варвара Петровна, а Степан Трофимович ни единым рублем в ней не участвовал. Напротив, весь доход с землицы оставлял у себя в кармане и, кроме того, разорил ее вконец, сдав ее в аренду какому-то промышленнику и, тихонько от Варвары Петровны, продав на сруб рощу, то есть главную ее ценность. Эту рощицу он уже давно продавал урывками. Вся она стоила по крайней мере тысяч восемь, а он взял за нее только пять. Но он иногда слишком много проигрывал в клубе, а просить у Варвары Петровны боялся. Она скрежетала зубами, когда наконец обо всем узнала. И вдруг теперь сынок извещал, что приедет сам продать свои владения во что бы ни стало, а отцу поручал неотлагательно позаботиться о продаже. Ясное дело, что при благородстве и бескорыстии Степана Трофимовича ему стало совестно пред се cher enfant[41] (которого он в последний раз видел целых девять лет тому назад, в Петербурге, студентом). Первоначально все имение могло стоить тысяч тринадцать или четырнадцать, теперь вряд ли кто бы дал за него и пять. Без сомнения, Степан Трофимович имел полное право, по смыслу формальной доверенности, продать лес и, поставив в счет тысячерублевый невозможный ежегодный доход, столько лет высылавшийся аккуратно, сильно оградить себя при расчете. Но Степан Трофимович был благороден, со стремлениями высшими. В голове его мелькнула одна удивительно красивая мысль: когда приедет Петруша, вдруг благородно выложить на стол самый высший maximum цены, то есть даже пятнадцать тысяч, без малейшего намека на высылавшиеся до сих пор суммы, и крепко-крепко, со слезами, прижать к груди се cher fils,[42] чем и покончить все счеты. Отдаленно и осторожно начал он развертывать эту картинку пред Варварой Петровной. Он намекал, что это даже придаст какой-то особый, благородный оттенок их дружеской связи… их «идее». Это выставило бы в таком бескорыстном и великодушном виде прежних отцов и вообще прежних людей сравнительно с новою легкомысленною и социальною молодежью. Много еще он говорил, но Варвара Петровна всё отмалчивалась. Наконец сухо объявила ему, что согласна купить их землю и даст за нее maximum цены, то есть тысяч шесть, семь (и за четыре можно было купить). Об остальных же восьми тысячах, улетевших с рощей, не сказала ни слова.',\n",
       "  'translation': 'Stepan Trofimovich\\'s estate, as we used to call it (about fifty souls by the old way of reckoning, and adjoining Skvoreshniki), was not his at all, but had belonged to his first wife, and so now to their son, Pyotr Stepanovich Verkhovensky. Stepan Trofimovich was merely the trustee, and thus, once the nestling was fully fledged, acted through a formal warrant as manager of the estate. For the young man it was a profitable deal: he received up to a thousand roubles a year from his father as income from the estate, while under the new regulations it did not yield as much as five hundred (and perhaps even less). God knows how such arrangements were set up. However, the entire thousand was sent by Varvara Petrovna, and Stepan Trofimovich did not contribute a single rouble to it. On the contrary, he pocketed all the income from this bit of land, and, furthermore, ruined it altogether by leasing it to some dealer and, in secret from Varvara Petrovna, selling the timber that was its main valuable asset. He had been selling this timber piecemeal for a long time. Its total worth was about eight thousand at least, yet he got only five for it. But he sometimes lost too much at the club, and was afraid to ask Varvara Petrovna. She ground her teeth when she finally learned of it all. And now the boy suddenly notified him that he was coming himself to sell his property at all costs, and charged his father with promptly arranging for the sale. It was clear that Stepan Trofimovich, being a lofty and disinterested man, felt ashamed before ce cher enfant (whom he had last seen as a student in Petersburg all of nine years earlier). Originally, the entire estate might have been worth some thirteen or fourteen thousand, but now it was unlikely that anyone would give five for it. Stepan Trofimovich undoubtedly had every right, in terms of the formal warrant, to sell the timber, and taking into account the impossible annual income of a thousand roubles, which had been sent punctually for so many years, could make a good defense of himself in any final settlement. But Stepan Trofimovich was noble and had lofty aspirations. A remarkably beautiful thought flashed in his head: to lay out nobly on the table, when Petrusha came, the highest maximum of the pricethat is, even fifteen thousand-without the slightest hint at the sums that had been sent previously, and then firmly, very firmly, with tears, to press ce cher fils to his heart, and so settle all accounts. He began remotely and cautiously unfolding this picture before Varvara Petrovna. He hinted that it would even add some special, noble tinge to their friendly connection ... to their \"idea.\" It would show former fathers and former people generally in such a disinterested and magnanimous light, as compared with the new frivolous and social youth. He said many other things, but Varvara Petrovna kept silent. At last she dryly informed him that she would agree to buy their land and would pay the maximum price for it-that is, six or seven thousand (even four would have been enough). Of the remaining eight thousand that had flown away with the timber, she did not say a word.',\n",
       "  'idx': 289,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9760987162590027,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Шатов? Это брат Дарьи Павловны…',\n",
       "  'translation': '\"Shatov? He is Darya Pavlovna\\'s brother...\"',\n",
       "  'idx': 528,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9759403467178345,\n",
       "  'translator': 'PV'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlist = sorted(translator_to_pars['PV'], key=lambda d: d['sim'], reverse=True) \n",
    "newlist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All\n",
      "PV 4017\n",
      "Garnett 4017\n",
      "Katz 4017\n",
      "McDuff 4017\n",
      "Hogarth 4017\n",
      "Total 20085\n",
      "\n",
      "Train\n",
      "PV 3547\n",
      "Garnett 3547\n",
      "Katz 3547\n",
      "McDuff 3547\n",
      "Hogarth 3547\n",
      "\n",
      "Holdout\n",
      "PV 470\n",
      "Garnett 470\n",
      "Katz 470\n",
      "McDuff 470\n",
      "Hogarth 470\n",
      "Train total:  17735\n",
      "Val/Test total:  2350\n",
      "\n",
      "train % =  0.8829972616380384\n",
      "holdout % =  0.11700273836196166\n"
     ]
    }
   ],
   "source": [
    "abs_total = 0\n",
    "print('\\nAll')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    both = len(translator_to_pars_holdout[k]) + len(translator_to_pars[k])\n",
    "    print(k, both)\n",
    "    abs_total += both\n",
    "print('Total', abs_total)\n",
    "\n",
    "train_total = 0\n",
    "min_class = 100000000000\n",
    "print('\\nTrain')\n",
    "for k in translator_to_pars.keys():\n",
    "    print(k, len(translator_to_pars[k]))\n",
    "    if len(translator_to_pars[k]) < min_class:\n",
    "        min_class = len(translator_to_pars[k])\n",
    "    \n",
    "train_total = len(translator_to_pars.keys()) * min_class\n",
    "\n",
    "holdout_total = 0\n",
    "min_class_h = 100000000000\n",
    "print('\\nHoldout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    print(k, len(translator_to_pars_holdout[k]))\n",
    "    if len(translator_to_pars_holdout[k]) < min_class_h:\n",
    "        min_class_h = len(translator_to_pars_holdout[k])\n",
    "\n",
    "holdout_total = len(translator_to_pars.keys()) * min_class_h\n",
    "\n",
    "print('Train total: ', min_class*5)\n",
    "print('Val/Test total: ', min_class_h*5)\n",
    "print()\n",
    "print('train % = ', train_total/(holdout_total+train_total))\n",
    "print('holdout % = ', holdout_total/(holdout_total+train_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 4 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(list(translator_to_pars.keys()))\n",
    "print(le.transform([\"Garnett\", \"McDuff\", \"PV\", \"Katz\", \"Hogarth\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "i = 0\n",
    "for tr in translator_to_pars.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat,  'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list.append(sent_dict)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "data_list_holdout = []\n",
    "i = 0\n",
    "for tr in translator_to_pars_holdout.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars_holdout[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat, 'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list_holdout.append(sent_dict)\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>book</th>\n",
       "      <th>labels</th>\n",
       "      <th>concat</th>\n",
       "      <th>translator</th>\n",
       "      <th>sim</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>Из лицея молодой человек в первые два года при...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976574</td>\n",
       "      <td>Из лицея молодой человек в первые два года при...</td>\n",
       "      <td>For the first two years the young man came hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1263</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>– Вещь короткая; даже, если хотите, по-настоящ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976535</td>\n",
       "      <td>– Вещь короткая; даже, если хотите, по-настоящ...</td>\n",
       "      <td>\"It's a short matter; in fact, if you like, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1228</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>– Да кто? Кто велел вам сюда приходить? – допр...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976279</td>\n",
       "      <td>– Да кто? Кто велел вам сюда приходить? – допр...</td>\n",
       "      <td>\"But, who? Who told you to come here?\" Varvara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>289</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>Так называемое у нас имение Степана Трофимович...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976099</td>\n",
       "      <td>Так называемое у нас имение Степана Трофимович...</td>\n",
       "      <td>Stepan Trofimovich's estate, as we used to cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>– Шатов? Это брат Дарьи Павловны… &lt;SEP&gt; \"Shato...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.975940</td>\n",
       "      <td>– Шатов? Это брат Дарьи Павловны…</td>\n",
       "      <td>\"Shatov? He is Darya Pavlovna's brother...\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx    book  labels                                             concat  \\\n",
       "0    91  Demons       4  Из лицея молодой человек в первые два года при...   \n",
       "1  1263  Demons       4  – Вещь короткая; даже, если хотите, по-настоящ...   \n",
       "2  1228  Demons       4  – Да кто? Кто велел вам сюда приходить? – допр...   \n",
       "3   289  Demons       4  Так называемое у нас имение Степана Трофимович...   \n",
       "4   528  Demons       4  – Шатов? Это брат Дарьи Павловны… <SEP> \"Shato...   \n",
       "\n",
       "  translator       sim                                                src  \\\n",
       "0         PV  0.976574  Из лицея молодой человек в первые два года при...   \n",
       "1         PV  0.976535  – Вещь короткая; даже, если хотите, по-настоящ...   \n",
       "2         PV  0.976279  – Да кто? Кто велел вам сюда приходить? – допр...   \n",
       "3         PV  0.976099  Так называемое у нас имение Степана Трофимович...   \n",
       "4         PV  0.975940                  – Шатов? Это брат Дарьи Павловны…   \n",
       "\n",
       "                                                 tgt  \n",
       "0  For the first two years the young man came hom...  \n",
       "1  \"It's a short matter; in fact, if you like, it...  \n",
       "2  \"But, who? Who told you to come here?\" Varvara...  \n",
       "3  Stepan Trofimovich's estate, as we used to cal...  \n",
       "4        \"Shatov? He is Darya Pavlovna's brother...\"  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df_holdout = pd.DataFrame(data_list_holdout)\n",
    "df_holdout_X = df_holdout[['idx','book', 'concat', 'translator', 'sim', 'src', 'tgt']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17735, 8)\n",
      "(2350, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  (17735, 8)\n",
      "val size:  (1175, 8)\n",
      "test size:  (1175, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_texts, val_texts, test_labels, val_labels = train_test_split(\n",
    "    df_holdout_X, df_holdout['labels'],\n",
    "    stratify = df_holdout['labels'], shuffle=True, test_size=0.5\n",
    ")\n",
    "\n",
    "aligned_train_df = df\n",
    "test_df = pd.concat([test_texts, test_labels], axis=1)\n",
    "val_df = pd.concat([val_texts, val_labels], axis=1)\n",
    "print('train size: ', aligned_train_df.shape)\n",
    "print('val size: ', val_df.shape)\n",
    "print('test size: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ALIGNED TRAIN\n",
    "aligned_train_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/aligned_train_df.pickle\")  \n",
    "\n",
    "# SAVE HOLDOUT VAL\n",
    "val_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/experiment_val_df.pickle\")  \n",
    "\n",
    "# SAVE HOLDOUT TEST\n",
    "test_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/experiment_test_df.pickle\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_paragraph_len = 20\n",
    "max_paragraph_len = 1000000000000\n",
    "top_k_percent = 1\n",
    "num_k = 5000\n",
    "drop_top = 0.00\n",
    "align_scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_books = ['TheIdiot', 'NotesFromUnderground']\n",
    "ignore_books = []\n",
    "translator_to_pars = {}\n",
    "translator_to_pars_holdout = {}\n",
    "\n",
    "# for each book in train:\n",
    "for book in sorted(list(aligned_paragraph_dataset.keys())):\n",
    "    # get par list of aligned sentences, best k alignments\n",
    "    book_par_list = [list(aligned_paragraph_dataset[book][p].values()) for p in range(len(aligned_paragraph_dataset[book]))]\n",
    "    source_par_list = source_paragraph_dataset[book]\n",
    "\n",
    "    if book in holdout_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, 1.0, 5000, 0, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    elif book not in ignore_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, top_k_percent, num_k, drop_top, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    else:\n",
    "        top_k = []\n",
    "\n",
    "    for i, sim in top_k:\n",
    "        par_trans_dict = aligned_paragraph_dataset[book][i]\n",
    "        par_source = source_paragraph_dataset[book][i]\n",
    "\n",
    "        for translator, t in par_trans_dict.items():\n",
    "            t = t.replace('\\\\\\'', '\\'')\n",
    "            datum_dict = {'source':par_source, 'translation': t, 'idx': i, 'book': book, 'sim': sim, 'translator': translator}\n",
    "\n",
    "            if translator not in translator_to_pars.keys():\n",
    "                translator_to_pars[translator] = []\n",
    "                translator_to_pars_holdout[translator] = []\n",
    "                \n",
    "            if book in holdout_books:\n",
    "                translator_to_pars_holdout[translator].append(datum_dict)\n",
    "                # print('len par_list: ', len(book_par_list))\n",
    "                # print('len top_k: ', len(top_k))\n",
    "            else:\n",
    "                translator_to_pars[translator].append(datum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entire_dataset = 0\n",
    "holdout_entire_dataset = 0\n",
    "for t in translator_to_pars.keys():\n",
    "    train_entire_dataset += len(translator_to_pars[t])\n",
    "    holdout_entire_dataset += len(translator_to_pars_holdout[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547\n",
      "470\n"
     ]
    }
   ],
   "source": [
    "min_len = int(aligned_train_df.shape[0]/5)\n",
    "print(min_len)\n",
    "for t in translator_to_pars.keys():\n",
    "    keep = sample(translator_to_pars[t], min_len) \n",
    "    translator_to_pars[t] = keep\n",
    "\n",
    "min_len_h = len(translator_to_pars_holdout['Hogarth'])\n",
    "print(min_len_h)\n",
    "for t in translator_to_pars_holdout.keys():\n",
    "    keep = sample(translator_to_pars_holdout[t], min_len_h) \n",
    "    translator_to_pars_holdout[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train + Holdout\n",
      "PV 4017\n",
      "Garnett 4017\n",
      "Katz 4017\n",
      "McDuff 4017\n",
      "Hogarth 4017\n",
      "Total 20085\n",
      "\n",
      "Train\n",
      "PV 3547\n",
      "Garnett 3547\n",
      "Katz 3547\n",
      "McDuff 3547\n",
      "Hogarth 3547\n",
      "\n",
      "Holdout\n",
      "PV 470\n",
      "Garnett 470\n",
      "Katz 470\n",
      "McDuff 470\n",
      "Hogarth 470\n",
      "Train total:  17735\n",
      "Val/Test total:  2350\n",
      "\n",
      "train % =  0.8829972616380384\n",
      "holdout % =  0.11700273836196166\n",
      "\n",
      "entire dataset % =  0.2643945975831293\n",
      "entire train % =  0.28937149197232737\n",
      "holdout train % =  0.160103556342826\n"
     ]
    }
   ],
   "source": [
    "abs_total = 0\n",
    "print('\\nTrain + Holdout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    both = len(translator_to_pars_holdout[k]) + len(translator_to_pars[k])\n",
    "    print(k, both)\n",
    "    abs_total += both\n",
    "print('Total', abs_total)\n",
    "\n",
    "train_total = 0\n",
    "min_class = 100000000000\n",
    "print('\\nTrain')\n",
    "for k in translator_to_pars.keys():\n",
    "    print(k, len(translator_to_pars[k]))\n",
    "    if len(translator_to_pars[k]) < min_class:\n",
    "        min_class = len(translator_to_pars[k])\n",
    "    \n",
    "train_total = len(translator_to_pars.keys()) * min_class\n",
    "\n",
    "holdout_total = 0\n",
    "min_class_h = 100000000000\n",
    "print('\\nHoldout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    print(k, len(translator_to_pars_holdout[k]))\n",
    "    if len(translator_to_pars_holdout[k]) < min_class_h:\n",
    "        min_class_h = len(translator_to_pars_holdout[k])\n",
    "\n",
    "holdout_total = len(translator_to_pars.keys()) * min_class_h\n",
    "\n",
    "print('Train total: ', min_class*5)\n",
    "print('Val/Test total: ', min_class_h*5)\n",
    "print()\n",
    "print('train % = ', train_total/(holdout_total+train_total))\n",
    "print('holdout % = ', holdout_total/(holdout_total+train_total))\n",
    "print()\n",
    "print('entire dataset % = ', (holdout_total+train_total)/(train_entire_dataset + holdout_entire_dataset))\n",
    "print('entire train % = ', (train_total)/(train_entire_dataset))\n",
    "print('holdout train % = ', (holdout_total)/(holdout_entire_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "i = 0\n",
    "for tr in translator_to_pars.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat,  'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list.append(sent_dict)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17735, 8)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data_list)\n",
    "random_train_df = df\n",
    "random_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ALIGNED TRAIN\n",
    "random_train_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/random_train_df.pickle\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
