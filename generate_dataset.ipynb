{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/sim_models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "import numpy as np\n",
    "\n",
    "class ParaModel(nn.Module):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        super(ParaModel, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab = vocab\n",
    "        self.gpu = args.gpu\n",
    "\n",
    "        self.cosine = CosineSimilarity()\n",
    "\n",
    "    def compute_mask(self, lengths):\n",
    "\n",
    "        lengths = lengths.cpu()\n",
    "        max_len = torch.max(lengths)\n",
    "        range_row = torch.arange(0, max_len).long()[None, :].expand(lengths.size()[0], max_len)\n",
    "        mask = lengths[:, None].expand_as(range_row)\n",
    "        mask = range_row < mask\n",
    "        mask = mask.float()\n",
    "        if self.gpu >= 0:\n",
    "            mask = mask.cuda()\n",
    "        return mask\n",
    "\n",
    "    def torchify_batch(self, batch):\n",
    "\n",
    "        max_len = 0\n",
    "        for i in batch:\n",
    "            if len(i.embeddings) > max_len:\n",
    "                max_len = len(i.embeddings)\n",
    "\n",
    "        batch_len = len(batch)\n",
    "\n",
    "        np_sents = np.zeros((batch_len, max_len), dtype='int32')\n",
    "        np_lens = np.zeros((batch_len,), dtype='int32')\n",
    "\n",
    "        for i, ex in enumerate(batch):\n",
    "            np_sents[i, :len(ex.embeddings)] = ex.embeddings\n",
    "            np_lens[i] = len(ex.embeddings)\n",
    "\n",
    "        idxs, lengths, masks = torch.from_numpy(np_sents).long(), \\\n",
    "                               torch.from_numpy(np_lens).float().long(), \\\n",
    "                               self.compute_mask(torch.from_numpy(np_lens).long())\n",
    "\n",
    "        if self.gpu >= 0:\n",
    "            idxs = idxs.cuda()\n",
    "            lengths = lengths.cuda()\n",
    "            masks = masks.cuda()\n",
    "    \n",
    "        return idxs, lengths, masks\n",
    "\n",
    "    def scoring_function(self, g_idxs1, g_mask1, g_lengths1, g_idxs2, g_mask2, g_lengths2):\n",
    "\n",
    "        g1 = self.encode(g_idxs1, g_mask1, g_lengths1)\n",
    "        g2 = self.encode(g_idxs2, g_mask2, g_lengths2)\n",
    "        return self.cosine(g1, g2)\n",
    "\n",
    "class WordAveraging(ParaModel):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        super(WordAveraging, self).__init__(args, vocab)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embedding = nn.Embedding(len(self.vocab), self.args.dim)\n",
    "\n",
    "        if args.gpu >= 0:\n",
    "           self.cuda()\n",
    "\n",
    "    def encode(self, idxs, mask, lengths):\n",
    "        word_embs = self.embedding(idxs)\n",
    "        word_embs = word_embs * mask[:, :, None]\n",
    "        g = word_embs.sum(dim=1) / lengths[:, None].float()\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/sim_utils.py\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_wordmap(textfile):\n",
    "    words={}\n",
    "    We = []\n",
    "    f = io.open(textfile, 'r', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    if len(lines[0].split()) == 2:\n",
    "        lines.pop(0)\n",
    "    ct = 0\n",
    "    for (n,i) in enumerate(lines):\n",
    "        word = i.split(' ', 1)[0]\n",
    "        vec = i.split(' ', 1)[1].split(' ')\n",
    "        j = 0\n",
    "        v = []\n",
    "        while j < len(vec):\n",
    "            v.append(float(vec[j]))\n",
    "            j += 1\n",
    "        words[word] = ct\n",
    "        ct += 1\n",
    "        We.append(v)\n",
    "    return words, np.array(We)\n",
    "\n",
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)\n",
    "\n",
    "def max_pool(x, lengths, gpu):\n",
    "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
    "    if gpu >= 0:\n",
    "        out = out.cuda()\n",
    "    for i in range(len(lengths)):\n",
    "        out[i] = torch.max(x[i][0:lengths[i]], 0)[0]\n",
    "    return out\n",
    "\n",
    "def mean_pool(x, lengths, gpu):\n",
    "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
    "    if gpu >= 0:\n",
    "        out = out.cuda()\n",
    "    for i in range(len(lengths)):\n",
    "        out[i] = torch.mean(x[i][0:lengths[i]], 0)\n",
    "    return out\n",
    "\n",
    "def lookup(words, w):\n",
    "    w = w.lower()\n",
    "    if w in words:\n",
    "        return words[w]\n",
    "\n",
    "class Example(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence.strip().lower()\n",
    "        self.embeddings = []\n",
    "        self.representation = None\n",
    "\n",
    "    def populate_embeddings(self, words):\n",
    "        sentence = self.sentence.lower()\n",
    "        arr = sentence.split()\n",
    "        for i in arr:\n",
    "            emb = lookup(words, i)\n",
    "            if emb:\n",
    "                self.embeddings.append(emb)\n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings.append(words['UUUNKKK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkatsy/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# %load par3/par3_align/similarity/test_sim.py\n",
    "import torch\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "tok = TreebankWordTokenizer()\n",
    "\n",
    "model = torch.load('/home/kkatsy/par3/par3_align/similarity/sim/sim.pt')\n",
    "state_dict = model['state_dict']\n",
    "vocab_words = model['vocab_words']\n",
    "args = model['args']\n",
    "# turn off gpu\n",
    "model = WordAveraging(args, vocab_words)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/kkatsy/par3/par3_align/similarity/sim/sim.sp.30k.model')\n",
    "model.eval()\n",
    "\n",
    "def make_example(sentence, model):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = \" \".join(tok.tokenize(sentence))\n",
    "    sentence = sp.EncodeAsPieces(sentence)\n",
    "    wp1 = Example(\" \".join(sentence))\n",
    "    wp1.populate_embeddings(model.vocab)\n",
    "    return wp1\n",
    "\n",
    "def find_similarity(s1, s2):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        s2 = [make_example(x, model) for x in s2]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        wx2, wl2, wm2 = model.torchify_batch(s2)\n",
    "        BATCH_SIZE = 512\n",
    "        all_scores = []\n",
    "        for i in range(0, len(wx1), BATCH_SIZE):\n",
    "            scores = model.scoring_function(wx1[i:i + BATCH_SIZE], wm1[i:i + BATCH_SIZE], wl1[i:i + BATCH_SIZE],\n",
    "                                            wx2[i:i + BATCH_SIZE], wm2[i:i + BATCH_SIZE], wl2[i:i + BATCH_SIZE])\n",
    "            all_scores.extend([x.item() for x in scores])\n",
    "        return all_scores\n",
    "\n",
    "def find_similarity_matrix(s1, s2):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        s2 = [make_example(x, model) for x in s2]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        wx2, wl2, wm2 = model.torchify_batch(s2)\n",
    "\n",
    "        BATCH_SIZE = 2000\n",
    "        vecs1 = []\n",
    "        vecs2 = []\n",
    "        for i in range(0, len(wx1), BATCH_SIZE):\n",
    "            curr_vecs1 = model.encode(idxs=wx1[i:i + BATCH_SIZE],\n",
    "                                      mask=wm1[i:i + BATCH_SIZE],\n",
    "                                      lengths=wl1[i:i + BATCH_SIZE])\n",
    "            vecs1.append(curr_vecs1)\n",
    "        for i in range(0, len(wx2), BATCH_SIZE):\n",
    "            curr_vecs2 = model.encode(idxs=wx2[i:i + BATCH_SIZE],\n",
    "                                      mask=wm2[i:i + BATCH_SIZE],\n",
    "                                      lengths=wl2[i:i + BATCH_SIZE])\n",
    "            vecs2.append(curr_vecs2)\n",
    "        vecs1 = torch.cat(vecs1)\n",
    "        vecs2 = torch.cat(vecs2)\n",
    "        dot_product = torch.matmul(vecs1, vecs2.t())\n",
    "\n",
    "        vecs1_norm = torch.norm(vecs1, dim=1, keepdim=True)\n",
    "        vecs2_norm = torch.norm(vecs2, dim=1, keepdim=True)\n",
    "        norm_product = torch.matmul(vecs1_norm, vecs2_norm.t())\n",
    "    return torch.div(dot_product, norm_product)\n",
    "\n",
    "def encode_text(s1):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        vecs1 = model.encode(idxs=wx1, mask=wm1, lengths=wl1)\n",
    "        return vecs1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordAveraging(\n",
       "  (cosine): CosineSimilarity()\n",
       "  (embedding): Embedding(65733, 300)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = TreebankWordTokenizer()\n",
    "\n",
    "model = torch.load('/home/kkatsy/par3/par3_align/similarity/sim/sim.pt')\n",
    "state_dict = model['state_dict']\n",
    "vocab_words = model['vocab_words']\n",
    "args = model['args']\n",
    "# turn off gpu\n",
    "model = WordAveraging(args, vocab_words)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/kkatsy/par3/par3_align/similarity/sim/sim.sp.30k.model')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(refs, cands, metric='sim'):\n",
    "    return find_similarity(refs,cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('aligned_paragraph_dataset.pickle', 'rb') as fp:\n",
    "  aligned_paragraph_dataset = pickle.load(fp)\n",
    "\n",
    "with open('source_paragraph_dataset.pickle', 'rb') as fp:\n",
    "  source_paragraph_dataset = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "\n",
    "def get_best_alignments(par_list, source_par_list, top_k_percent, num_k, drop_top, metric, min_len, max_len, align_scale):\n",
    "\n",
    "    # dict -> score:par_set\n",
    "    # iter thru par_list, prune by length, get metric for set\n",
    "    keep_index_list = []\n",
    "    i2score = {}\n",
    "    for i in range(len(par_list)):\n",
    "        keep_index_list.append(i)\n",
    "        par_set = par_list[i]\n",
    "\n",
    "        max_par_len = len(max(par_set, key = len))\n",
    "        min_par_len = len(min(par_set, key = len))\n",
    "        source_len = len(source_par_list[i])\n",
    "\n",
    "        if (min_par_len >= min_len) and (max_par_len) <= max_len and not all(x==par_set[0] for x in par_set) and (max_par_len <= align_scale*source_len) and (min_par_len*align_scale >= source_len):\n",
    "\n",
    "            pairs = list(itertools.combinations(par_set, 2))\n",
    "            refs, cands = [], []\n",
    "            for s1, s2 in pairs:\n",
    "                refs.append(s1)\n",
    "                cands.append(s2)\n",
    "                \n",
    "            pair_scores = get_score(refs, cands, metric)\n",
    "\n",
    "            average_score = mean(pair_scores)\n",
    "            i2score[i] = average_score\n",
    "\n",
    "    # get top k par sets\n",
    "    num_pars = len(list(i2score))\n",
    "    top_k = int(top_k_percent * num_pars)\n",
    "    if top_k >= num_k:\n",
    "        top_k_scores = sorted(i2score.items(), key=itemgetter(1), reverse=True)[int(num_pars*drop_top):int(num_pars*drop_top) + num_k]\n",
    "    else:\n",
    "        top_k_scores = sorted(i2score.items(), key=itemgetter(1), reverse=True)[int(num_pars*drop_top):int(num_pars*drop_top) + top_k]\n",
    "    \n",
    "    i2score = sorted(i2score.items(), key=itemgetter(1), reverse=True)\n",
    "    return i2score, top_k_scores, keep_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_paragraph_len = 20\n",
    "max_paragraph_len = 1000000000000\n",
    "top_k_percent = 0.9\n",
    "num_k = 5000\n",
    "drop_top = 0.02\n",
    "align_scale = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout\n",
    "# NotesFromUnderground - Katz, PV, Garnett, Hogarth\n",
    "# PoorFolk - McDuff, Hogarth, Garnett\n",
    "# TheIdiot - Garnett, McDuff, PV\n",
    "# CrimeAndPunishment - Katz, McDuff, PV, Garnett\n",
    "holdout_books = ['TheIdiot', 'NotesFromUnderground']\n",
    "ignore_books = []\n",
    "translator_to_pars = {}\n",
    "translator_to_pars_holdout = {}\n",
    "\n",
    "# for each book in train:\n",
    "for book in sorted(list(aligned_paragraph_dataset.keys())):\n",
    "    # get par list of aligned sentences, best k alignments\n",
    "    book_par_list = [list(aligned_paragraph_dataset[book][p].values()) for p in range(len(aligned_paragraph_dataset[book]))]\n",
    "    source_par_list = source_paragraph_dataset[book]\n",
    "\n",
    "    if book in holdout_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, 1.0, 5000, 0, 'sim', min_paragraph_len, max_paragraph_len, 1)\n",
    "    elif book not in ignore_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, top_k_percent, num_k, drop_top, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    else:\n",
    "        top_k = []\n",
    "\n",
    "    for i, sim in top_k:\n",
    "        par_trans_dict = aligned_paragraph_dataset[book][i]\n",
    "        par_source = source_paragraph_dataset[book][i]\n",
    "\n",
    "        for translator, t in par_trans_dict.items():\n",
    "            t = t.replace('\\\\\\'', '\\'')\n",
    "            datum_dict = {'source':par_source, 'translation': t, 'idx': i, 'book': book, 'sim': sim, 'translator': translator}\n",
    "\n",
    "            if translator not in translator_to_pars.keys():\n",
    "                translator_to_pars[translator] = []\n",
    "                translator_to_pars_holdout[translator] = []\n",
    "                \n",
    "            if book in holdout_books:\n",
    "                translator_to_pars_holdout[translator].append(datum_dict)\n",
    "                # print('len par_list: ', len(book_par_list))\n",
    "                # print('len top_k: ', len(top_k))\n",
    "            else:\n",
    "                translator_to_pars[translator].append(datum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547\n"
     ]
    }
   ],
   "source": [
    "min_len = len(translator_to_pars['Hogarth'])\n",
    "print(min_len)\n",
    "for t in translator_to_pars.keys():\n",
    "    keep = sorted(translator_to_pars[t], key=lambda d: d['sim'], reverse=True)[:min_len]\n",
    "    translator_to_pars[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "min_len_h = len(translator_to_pars_holdout['Hogarth'])\n",
    "print(min_len_h)\n",
    "for t in translator_to_pars_holdout.keys():\n",
    "    keep = sample(translator_to_pars_holdout[t], min_len_h) \n",
    "    translator_to_pars_holdout[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All\n",
      "PV 3980\n",
      "Garnett 3980\n",
      "Katz 3980\n",
      "McDuff 3980\n",
      "Hogarth 3980\n",
      "Total 19900\n",
      "\n",
      "Train\n",
      "PV 3547\n",
      "Garnett 3547\n",
      "Katz 3547\n",
      "McDuff 3547\n",
      "Hogarth 3547\n",
      "\n",
      "Holdout\n",
      "PV 433\n",
      "Garnett 433\n",
      "Katz 433\n",
      "McDuff 433\n",
      "Hogarth 433\n",
      "Train total:  17735\n",
      "Val/Test total:  2165\n",
      "\n",
      "train % =  0.8912060301507537\n",
      "holdout % =  0.10879396984924623\n"
     ]
    }
   ],
   "source": [
    "abs_total = 0\n",
    "print('\\nAll')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    both = len(translator_to_pars_holdout[k]) + len(translator_to_pars[k])\n",
    "    print(k, both)\n",
    "    abs_total += both\n",
    "print('Total', abs_total)\n",
    "\n",
    "train_total = 0\n",
    "min_class = 100000000000\n",
    "print('\\nTrain')\n",
    "for k in translator_to_pars.keys():\n",
    "    print(k, len(translator_to_pars[k]))\n",
    "    if len(translator_to_pars[k]) < min_class:\n",
    "        min_class = len(translator_to_pars[k])\n",
    "    \n",
    "train_total = len(translator_to_pars.keys()) * min_class\n",
    "\n",
    "holdout_total = 0\n",
    "min_class_h = 100000000000\n",
    "print('\\nHoldout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    print(k, len(translator_to_pars_holdout[k]))\n",
    "    if len(translator_to_pars_holdout[k]) < min_class_h:\n",
    "        min_class_h = len(translator_to_pars_holdout[k])\n",
    "\n",
    "holdout_total = len(translator_to_pars.keys()) * min_class_h\n",
    "\n",
    "print('Train total: ', min_class*5)\n",
    "print('Val/Test total: ', min_class_h*5)\n",
    "print()\n",
    "print('train % = ', train_total/(holdout_total+train_total))\n",
    "print('holdout % = ', holdout_total/(holdout_total+train_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 4 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(list(translator_to_pars.keys()))\n",
    "print(le.transform([\"Garnett\", \"McDuff\", \"PV\", \"Katz\", \"Hogarth\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "i = 0\n",
    "for tr in translator_to_pars.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat,  'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list.append(sent_dict)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "data_list_holdout = []\n",
    "i = 0\n",
    "for tr in translator_to_pars_holdout.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars_holdout[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat, 'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list_holdout.append(sent_dict)\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>book</th>\n",
       "      <th>labels</th>\n",
       "      <th>concat</th>\n",
       "      <th>translator</th>\n",
       "      <th>sim</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>Из лицея молодой человек в первые два года при...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976574</td>\n",
       "      <td>Из лицея молодой человек в первые два года при...</td>\n",
       "      <td>For the first two years the young man came hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1263</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>– Вещь короткая; даже, если хотите, по-настоящ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976535</td>\n",
       "      <td>– Вещь короткая; даже, если хотите, по-настоящ...</td>\n",
       "      <td>\"It's a short matter; in fact, if you like, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1228</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>– Да кто? Кто велел вам сюда приходить? – допр...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976279</td>\n",
       "      <td>– Да кто? Кто велел вам сюда приходить? – допр...</td>\n",
       "      <td>\"But, who? Who told you to come here?\" Varvara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>289</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>Так называемое у нас имение Степана Трофимович...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976099</td>\n",
       "      <td>Так называемое у нас имение Степана Трофимович...</td>\n",
       "      <td>Stepan Trofimovich's estate, as we used to cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528</td>\n",
       "      <td>Demons</td>\n",
       "      <td>4</td>\n",
       "      <td>– Шатов? Это брат Дарьи Павловны… &lt;SEP&gt; \"Shato...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.975940</td>\n",
       "      <td>– Шатов? Это брат Дарьи Павловны…</td>\n",
       "      <td>\"Shatov? He is Darya Pavlovna's brother...\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx    book  labels                                             concat  \\\n",
       "0    91  Demons       4  Из лицея молодой человек в первые два года при...   \n",
       "1  1263  Demons       4  – Вещь короткая; даже, если хотите, по-настоящ...   \n",
       "2  1228  Demons       4  – Да кто? Кто велел вам сюда приходить? – допр...   \n",
       "3   289  Demons       4  Так называемое у нас имение Степана Трофимович...   \n",
       "4   528  Demons       4  – Шатов? Это брат Дарьи Павловны… <SEP> \"Shato...   \n",
       "\n",
       "  translator       sim                                                src  \\\n",
       "0         PV  0.976574  Из лицея молодой человек в первые два года при...   \n",
       "1         PV  0.976535  – Вещь короткая; даже, если хотите, по-настоящ...   \n",
       "2         PV  0.976279  – Да кто? Кто велел вам сюда приходить? – допр...   \n",
       "3         PV  0.976099  Так называемое у нас имение Степана Трофимович...   \n",
       "4         PV  0.975940                  – Шатов? Это брат Дарьи Павловны…   \n",
       "\n",
       "                                                 tgt  \n",
       "0  For the first two years the young man came hom...  \n",
       "1  \"It's a short matter; in fact, if you like, it...  \n",
       "2  \"But, who? Who told you to come here?\" Varvara...  \n",
       "3  Stepan Trofimovich's estate, as we used to cal...  \n",
       "4        \"Shatov? He is Darya Pavlovna's brother...\"  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df_holdout = pd.DataFrame(data_list_holdout)\n",
    "df_holdout_X = df_holdout[['idx','book', 'concat', 'translator', 'sim', 'src', 'tgt']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17735, 8)\n",
      "(2165, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  (17735, 8)\n",
      "val size:  (1083, 8)\n",
      "test size:  (1082, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_texts, val_texts, test_labels, val_labels = train_test_split(\n",
    "    df_holdout_X, df_holdout['labels'],\n",
    "    stratify = df_holdout['labels'], shuffle=True, test_size=0.5\n",
    ")\n",
    "\n",
    "aligned_train_df = df\n",
    "test_df = pd.concat([test_texts, test_labels], axis=1)\n",
    "val_df = pd.concat([val_texts, val_labels], axis=1)\n",
    "print('train size: ', aligned_train_df.shape)\n",
    "print('val size: ', val_df.shape)\n",
    "print('test size: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ALIGNED TRAIN\n",
    "aligned_train_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/aligned_train_df.pickle\")  \n",
    "\n",
    "# SAVE HOLDOUT VAL\n",
    "val_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/experiment_val_df.pickle\")  \n",
    "\n",
    "# SAVE HOLDOUT TEST\n",
    "test_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/experiment_test_df.pickle\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_paragraph_len = 20\n",
    "max_paragraph_len = 1000000000000\n",
    "top_k_percent = 1\n",
    "num_k = 5000\n",
    "drop_top = 0.00\n",
    "align_scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_books = ['TheIdiot', 'NotesFromUnderground']\n",
    "ignore_books = []\n",
    "translator_to_pars = {}\n",
    "translator_to_pars_holdout = {}\n",
    "\n",
    "# for each book in train:\n",
    "for book in sorted(list(aligned_paragraph_dataset.keys())):\n",
    "    # get par list of aligned sentences, best k alignments\n",
    "    book_par_list = [list(aligned_paragraph_dataset[book][p].values()) for p in range(len(aligned_paragraph_dataset[book]))]\n",
    "    source_par_list = source_paragraph_dataset[book]\n",
    "\n",
    "    if book in holdout_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, 1.0, 5000, 0, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    elif book not in ignore_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, top_k_percent, num_k, drop_top, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    else:\n",
    "        top_k = []\n",
    "\n",
    "    for i, sim in top_k:\n",
    "        par_trans_dict = aligned_paragraph_dataset[book][i]\n",
    "        par_source = source_paragraph_dataset[book][i]\n",
    "\n",
    "        for translator, t in par_trans_dict.items():\n",
    "            t = t.replace('\\\\\\'', '\\'')\n",
    "            datum_dict = {'source':par_source, 'translation': t, 'idx': i, 'book': book, 'sim': sim, 'translator': translator}\n",
    "\n",
    "            if translator not in translator_to_pars.keys():\n",
    "                translator_to_pars[translator] = []\n",
    "                translator_to_pars_holdout[translator] = []\n",
    "                \n",
    "            if book in holdout_books:\n",
    "                translator_to_pars_holdout[translator].append(datum_dict)\n",
    "                # print('len par_list: ', len(book_par_list))\n",
    "                # print('len top_k: ', len(top_k))\n",
    "            else:\n",
    "                translator_to_pars[translator].append(datum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entire_dataset = 0\n",
    "holdout_entire_dataset = 0\n",
    "for t in translator_to_pars.keys():\n",
    "    train_entire_dataset += len(translator_to_pars[t])\n",
    "    holdout_entire_dataset += len(translator_to_pars_holdout[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547\n",
      "433\n"
     ]
    }
   ],
   "source": [
    "min_len = int(aligned_train_df.shape[0]/5)\n",
    "print(min_len)\n",
    "for t in translator_to_pars.keys():\n",
    "    keep = sample(translator_to_pars[t], min_len) \n",
    "    translator_to_pars[t] = keep\n",
    "\n",
    "min_len_h = len(translator_to_pars_holdout['Hogarth'])\n",
    "print(min_len_h)\n",
    "for t in translator_to_pars_holdout.keys():\n",
    "    keep = sample(translator_to_pars_holdout[t], min_len_h) \n",
    "    translator_to_pars_holdout[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train + Holdout\n",
      "PV 3980\n",
      "Garnett 3980\n",
      "Katz 3980\n",
      "McDuff 3980\n",
      "Hogarth 3980\n",
      "Total 19900\n",
      "\n",
      "Train\n",
      "PV 3547\n",
      "Garnett 3547\n",
      "Katz 3547\n",
      "McDuff 3547\n",
      "Hogarth 3547\n",
      "\n",
      "Holdout\n",
      "PV 433\n",
      "Garnett 433\n",
      "Katz 433\n",
      "McDuff 433\n",
      "Hogarth 433\n",
      "Train total:  17735\n",
      "Val/Test total:  2165\n",
      "\n",
      "train % =  0.8912060301507537\n",
      "holdout % =  0.10879396984924623\n",
      "\n",
      "entire dataset % =  0.27378790380276263\n",
      "entire train % =  0.30460470947907187\n",
      "holdout train % =  0.14971302122951385\n"
     ]
    }
   ],
   "source": [
    "abs_total = 0\n",
    "print('\\nTrain + Holdout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    both = len(translator_to_pars_holdout[k]) + len(translator_to_pars[k])\n",
    "    print(k, both)\n",
    "    abs_total += both\n",
    "print('Total', abs_total)\n",
    "\n",
    "train_total = 0\n",
    "min_class = 100000000000\n",
    "print('\\nTrain')\n",
    "for k in translator_to_pars.keys():\n",
    "    print(k, len(translator_to_pars[k]))\n",
    "    if len(translator_to_pars[k]) < min_class:\n",
    "        min_class = len(translator_to_pars[k])\n",
    "    \n",
    "train_total = len(translator_to_pars.keys()) * min_class\n",
    "\n",
    "holdout_total = 0\n",
    "min_class_h = 100000000000\n",
    "print('\\nHoldout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    print(k, len(translator_to_pars_holdout[k]))\n",
    "    if len(translator_to_pars_holdout[k]) < min_class_h:\n",
    "        min_class_h = len(translator_to_pars_holdout[k])\n",
    "\n",
    "holdout_total = len(translator_to_pars.keys()) * min_class_h\n",
    "\n",
    "print('Train total: ', min_class*5)\n",
    "print('Val/Test total: ', min_class_h*5)\n",
    "print()\n",
    "print('train % = ', train_total/(holdout_total+train_total))\n",
    "print('holdout % = ', holdout_total/(holdout_total+train_total))\n",
    "print()\n",
    "print('entire dataset % = ', (holdout_total+train_total)/(train_entire_dataset + holdout_entire_dataset))\n",
    "print('entire train % = ', (train_total)/(train_entire_dataset))\n",
    "print('holdout train % = ', (holdout_total)/(holdout_entire_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "i = 0\n",
    "for tr in translator_to_pars.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat,  'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list.append(sent_dict)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17735, 8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data_list)\n",
    "random_train_df = df\n",
    "random_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ALIGNED TRAIN\n",
    "random_train_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/random_train_df.pickle\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
