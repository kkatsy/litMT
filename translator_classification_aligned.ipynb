{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/sim_models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "import numpy as np\n",
    "\n",
    "class ParaModel(nn.Module):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        super(ParaModel, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab = vocab\n",
    "        self.gpu = args.gpu\n",
    "\n",
    "        self.cosine = CosineSimilarity()\n",
    "\n",
    "    def compute_mask(self, lengths):\n",
    "\n",
    "        lengths = lengths.cpu()\n",
    "        max_len = torch.max(lengths)\n",
    "        range_row = torch.arange(0, max_len).long()[None, :].expand(lengths.size()[0], max_len)\n",
    "        mask = lengths[:, None].expand_as(range_row)\n",
    "        mask = range_row < mask\n",
    "        mask = mask.float()\n",
    "        if self.gpu >= 0:\n",
    "            mask = mask.cuda()\n",
    "        return mask\n",
    "\n",
    "    def torchify_batch(self, batch):\n",
    "\n",
    "        max_len = 0\n",
    "        for i in batch:\n",
    "            if len(i.embeddings) > max_len:\n",
    "                max_len = len(i.embeddings)\n",
    "\n",
    "        batch_len = len(batch)\n",
    "\n",
    "        np_sents = np.zeros((batch_len, max_len), dtype='int32')\n",
    "        np_lens = np.zeros((batch_len,), dtype='int32')\n",
    "\n",
    "        for i, ex in enumerate(batch):\n",
    "            np_sents[i, :len(ex.embeddings)] = ex.embeddings\n",
    "            np_lens[i] = len(ex.embeddings)\n",
    "\n",
    "        idxs, lengths, masks = torch.from_numpy(np_sents).long(), \\\n",
    "                               torch.from_numpy(np_lens).float().long(), \\\n",
    "                               self.compute_mask(torch.from_numpy(np_lens).long())\n",
    "\n",
    "        if self.gpu >= 0:\n",
    "            idxs = idxs.cuda()\n",
    "            lengths = lengths.cuda()\n",
    "            masks = masks.cuda()\n",
    "    \n",
    "        return idxs, lengths, masks\n",
    "\n",
    "    def scoring_function(self, g_idxs1, g_mask1, g_lengths1, g_idxs2, g_mask2, g_lengths2):\n",
    "\n",
    "        g1 = self.encode(g_idxs1, g_mask1, g_lengths1)\n",
    "        g2 = self.encode(g_idxs2, g_mask2, g_lengths2)\n",
    "        return self.cosine(g1, g2)\n",
    "\n",
    "class WordAveraging(ParaModel):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        super(WordAveraging, self).__init__(args, vocab)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embedding = nn.Embedding(len(self.vocab), self.args.dim)\n",
    "\n",
    "        if args.gpu >= 0:\n",
    "           self.cuda()\n",
    "\n",
    "    def encode(self, idxs, mask, lengths):\n",
    "        word_embs = self.embedding(idxs)\n",
    "        word_embs = word_embs * mask[:, :, None]\n",
    "        g = word_embs.sum(dim=1) / lengths[:, None].float()\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/sim_utils.py\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_wordmap(textfile):\n",
    "    words={}\n",
    "    We = []\n",
    "    f = io.open(textfile, 'r', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    if len(lines[0].split()) == 2:\n",
    "        lines.pop(0)\n",
    "    ct = 0\n",
    "    for (n,i) in enumerate(lines):\n",
    "        word = i.split(' ', 1)[0]\n",
    "        vec = i.split(' ', 1)[1].split(' ')\n",
    "        j = 0\n",
    "        v = []\n",
    "        while j < len(vec):\n",
    "            v.append(float(vec[j]))\n",
    "            j += 1\n",
    "        words[word] = ct\n",
    "        ct += 1\n",
    "        We.append(v)\n",
    "    return words, np.array(We)\n",
    "\n",
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)\n",
    "\n",
    "def max_pool(x, lengths, gpu):\n",
    "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
    "    if gpu >= 0:\n",
    "        out = out.cuda()\n",
    "    for i in range(len(lengths)):\n",
    "        out[i] = torch.max(x[i][0:lengths[i]], 0)[0]\n",
    "    return out\n",
    "\n",
    "def mean_pool(x, lengths, gpu):\n",
    "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
    "    if gpu >= 0:\n",
    "        out = out.cuda()\n",
    "    for i in range(len(lengths)):\n",
    "        out[i] = torch.mean(x[i][0:lengths[i]], 0)\n",
    "    return out\n",
    "\n",
    "def lookup(words, w):\n",
    "    w = w.lower()\n",
    "    if w in words:\n",
    "        return words[w]\n",
    "\n",
    "class Example(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence.strip().lower()\n",
    "        self.embeddings = []\n",
    "        self.representation = None\n",
    "\n",
    "    def populate_embeddings(self, words):\n",
    "        sentence = self.sentence.lower()\n",
    "        arr = sentence.split()\n",
    "        for i in arr:\n",
    "            emb = lookup(words, i)\n",
    "            if emb:\n",
    "                self.embeddings.append(emb)\n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings.append(words['UUUNKKK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkatsy/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# %load par3/par3_align/similarity/test_sim.py\n",
    "import torch\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "tok = TreebankWordTokenizer()\n",
    "\n",
    "model = torch.load('/home/kkatsy/par3/par3_align/similarity/sim/sim.pt')\n",
    "state_dict = model['state_dict']\n",
    "vocab_words = model['vocab_words']\n",
    "args = model['args']\n",
    "# turn off gpu\n",
    "model = WordAveraging(args, vocab_words)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/kkatsy/par3/par3_align/similarity/sim/sim.sp.30k.model')\n",
    "model.eval()\n",
    "\n",
    "def make_example(sentence, model):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = \" \".join(tok.tokenize(sentence))\n",
    "    sentence = sp.EncodeAsPieces(sentence)\n",
    "    wp1 = Example(\" \".join(sentence))\n",
    "    wp1.populate_embeddings(model.vocab)\n",
    "    return wp1\n",
    "\n",
    "def find_similarity(s1, s2):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        s2 = [make_example(x, model) for x in s2]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        wx2, wl2, wm2 = model.torchify_batch(s2)\n",
    "        BATCH_SIZE = 512\n",
    "        all_scores = []\n",
    "        for i in range(0, len(wx1), BATCH_SIZE):\n",
    "            scores = model.scoring_function(wx1[i:i + BATCH_SIZE], wm1[i:i + BATCH_SIZE], wl1[i:i + BATCH_SIZE],\n",
    "                                            wx2[i:i + BATCH_SIZE], wm2[i:i + BATCH_SIZE], wl2[i:i + BATCH_SIZE])\n",
    "            all_scores.extend([x.item() for x in scores])\n",
    "        return all_scores\n",
    "\n",
    "def find_similarity_matrix(s1, s2):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        s2 = [make_example(x, model) for x in s2]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        wx2, wl2, wm2 = model.torchify_batch(s2)\n",
    "\n",
    "        BATCH_SIZE = 2000\n",
    "        vecs1 = []\n",
    "        vecs2 = []\n",
    "        for i in range(0, len(wx1), BATCH_SIZE):\n",
    "            curr_vecs1 = model.encode(idxs=wx1[i:i + BATCH_SIZE],\n",
    "                                      mask=wm1[i:i + BATCH_SIZE],\n",
    "                                      lengths=wl1[i:i + BATCH_SIZE])\n",
    "            vecs1.append(curr_vecs1)\n",
    "        for i in range(0, len(wx2), BATCH_SIZE):\n",
    "            curr_vecs2 = model.encode(idxs=wx2[i:i + BATCH_SIZE],\n",
    "                                      mask=wm2[i:i + BATCH_SIZE],\n",
    "                                      lengths=wl2[i:i + BATCH_SIZE])\n",
    "            vecs2.append(curr_vecs2)\n",
    "        vecs1 = torch.cat(vecs1)\n",
    "        vecs2 = torch.cat(vecs2)\n",
    "        dot_product = torch.matmul(vecs1, vecs2.t())\n",
    "\n",
    "        vecs1_norm = torch.norm(vecs1, dim=1, keepdim=True)\n",
    "        vecs2_norm = torch.norm(vecs2, dim=1, keepdim=True)\n",
    "        norm_product = torch.matmul(vecs1_norm, vecs2_norm.t())\n",
    "    return torch.div(dot_product, norm_product)\n",
    "\n",
    "def encode_text(s1):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        vecs1 = model.encode(idxs=wx1, mask=wm1, lengths=wl1)\n",
    "        return vecs1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordAveraging(\n",
       "  (cosine): CosineSimilarity()\n",
       "  (embedding): Embedding(65733, 300)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = TreebankWordTokenizer()\n",
    "\n",
    "model = torch.load('/home/kkatsy/par3/par3_align/similarity/sim/sim.pt')\n",
    "state_dict = model['state_dict']\n",
    "vocab_words = model['vocab_words']\n",
    "args = model['args']\n",
    "# turn off gpu\n",
    "model = WordAveraging(args, vocab_words)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/kkatsy/par3/par3_align/similarity/sim/sim.sp.30k.model')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(refs, cands, metric='sim'):\n",
    "    return find_similarity(refs,cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('aligned_paragraph_dataset.pickle', 'rb') as fp:\n",
    "  aligned_paragraph_dataset = pickle.load(fp)\n",
    "\n",
    "with open('source_paragraph_dataset.pickle', 'rb') as fp:\n",
    "  source_paragraph_dataset = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Katz': '\"You\\'re a scoundrel,\" the thought flashed through my mind, \"if you laugh at that now.\\'',\n",
       "  'PV': '\"What a scoundrel you are,\" raced through my head, \"to laugh at that now!\"',\n",
       "  'Garnett': '\"You are a scoundrel,\" a thought flashed through my mind, \"if you laugh at this now.\"',\n",
       "  'Hogarth': '\"You cur! \" I went on, in objurgation of Zvierkov.'},\n",
       " {'Katz': '\"So what!\" I cried in reply. \"Everything is lost now, anyway!\"',\n",
       "  'PV': '\"What of it!\" I cried, answering myself. \"All is lost now!\"',\n",
       "  'Garnett': '\"No matter!\" I cried, answering myself. \"Now everything is lost!\"',\n",
       "  'Hogarth': '\"What if you should get the laugh of me even now?\" \"No matter if he does,\" I cried the next moment, in answer to myself. \" Hy now you have lost everything.\"'},\n",
       " {'Katz': \"There was no sign of them, but it didn't matter. I knew where they were going.\",\n",
       "  'PV': 'Their trail was already cold; but no matter: I knew where they had gone.',\n",
       "  'Garnett': 'There was no trace to be seen of them, but that made no difference—I knew where they had gone.',\n",
       "  'Hogarth': 'The scent of my quarry had now had time to grow cool, yet I knew where to find the man for whom I was seeking.'},\n",
       " {'Katz': 'At the entrance stood a solitary, late-night cabby in a coarse peasant coat powdered with wet, seemingly warm snow that was still falling. It was steamy and stuffy outside. The little shaggy piebald nag was also dusted with snow and was coughing; I remember that very well. I headed for the rough-hewn sledge; but as  soon as I raised one foot to get in, the recollection of how Simonov had just given me six rubles hit me with such force that I tumbled into the sledge like a sack.',\n",
       "  'PV': 'By the porch stood a lonely cabbie, a night coachman, in a homespun coat all dusted with the still-falling wet and as if warm snow. It was steamy and stuffy. His shaggy little piebald nag was also all dusted with snow, and was coughing—I very much remember that. I rushed to the bast-covered sled; but as I raised my foot to get in, the recollection of the way Simonov had just given me the six roubles cut me down, and I dropped into the sled like a sack.',\n",
       "  'Garnett': 'At the steps was standing a solitary night sledge-driver in a rough peasant coat, powdered over with the still falling, wet, and as it were warm, snow. It was hot and steamy. The little shaggy piebald horse was also covered with snow and coughing, I remember that very well. I made a rush for the roughly made sledge; but as soon as I raised my foot to get into it, the recollection of how Simonov had just given me six roubles seemed to double me up and I tumbled into the sledge like a sack.',\n",
       "  'Hogarth': 'At the door of the hotel there was lounging a solitary night cabman, with his greatcoat dusted over with the moist — almost warm- snow which was still falling and making everything look dim and oppressive; while his shaggy little piebald pony also had a powdering of snow upon its back, and from time to time, I remember, kept coughing. Leaping into the rough bark sledge, I was in the act of bending my legs to sit down when the recollection of Simonov’s recent loan of six roubles threw me into such a state of mind that I rolled backwards on to the seat like a sack. \"'},\n",
       " {'Katz': '\"No! There\\'s a lot I have to do to make up for that!\" I cried. \"But make up for it I will or else I\\'ll perish on the spot this very night. Let\\'s go!\"',\n",
       "  'PV': '\"No! Much must be done to redeem it all!\" I cried out, \"but I will redeem it, or perish on the spot this very night! Drive!\"',\n",
       "  'Garnett': '\"No, I must do a great deal to make up for all that,\" I cried. \"But I will make up for it or perish on the spot this very night. Start!\"',\n",
       "  'Hogarth': 'Leaping into the rough bark sledge, I was in the act of bending my legs to sit down when the recollection of Simonov’s recent loan of six roubles threw me into such a state of mind that I rolled backwards on to the seat like a sack. \" Yes, it will take a great deal to set all this right,\" I exclaimed as I did so. \"Yet I \\'.rill set things right, or perish in the attempt. Forward! \"'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_paragraph_dataset['NotesFromUnderground'][260:265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«Подлец ты!— пронеслось в моей голове,— коли над этим теперь смеешься».',\n",
       " '—Пусть!— крикнул я, отвечая себе.— Теперь ведь уж все погибло!',\n",
       " 'Их уж и след простыл; но все равно: я знал, куда они поехали.',\n",
       " 'У крыльца стоял одинокий Ванька, ночник, в сермяге, весь запорошенный все еще валившимся мокрым и как будто теплым снегом. Было парно и душно. Маленькая лохматая, пегая лошаденка его была тоже вся запорошена и кашляла; я это очень помню. Я бросился в лубошные санки; но только было я занес ногу, чтоб сесть, воспоминание о том, как Симонов сейчас давал мне шесть рублей, так и подкосило меня, и я, как мешок, повалился в санки.',\n",
       " '—Нет! Надо много сделать, чтоб все это выкупить!— прокричал я,— но я выкуплю или в эту же ночь погибну на месте. Пошел!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_paragraph_dataset['NotesFromUnderground'][260:265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "\n",
    "def get_best_alignments(par_list, source_par_list, top_k_percent, num_k, drop_top, metric, min_len, max_len):\n",
    "\n",
    "    # dict -> score:par_set\n",
    "    # iter thru par_list, prune by length, get metric for set\n",
    "    keep_index_list = []\n",
    "    i2score = {}\n",
    "    for i in range(len(par_list)):\n",
    "        keep_index_list.append(i)\n",
    "        par_set = par_list[i]\n",
    "\n",
    "        max_par_len = len(max(par_set, key = len))\n",
    "        min_par_len = len(min(par_set, key = len))\n",
    "        source_len = len(source_par_list[i])\n",
    "\n",
    "        if (min_par_len >= min_len) and (max_par_len) <= max_len and not all(x==par_set[0] for x in par_set) and (max_par_len <= 3*source_len) and (min_par_len*3 >= source_len):\n",
    "\n",
    "            pairs = list(itertools.combinations(par_set, 2))\n",
    "            refs, cands = [], []\n",
    "            for s1, s2 in pairs:\n",
    "                refs.append(s1)\n",
    "                cands.append(s2)\n",
    "                \n",
    "            pair_scores = get_score(refs, cands, metric)\n",
    "\n",
    "            average_score = mean(pair_scores)\n",
    "            i2score[i] = average_score\n",
    "\n",
    "    # get top k par sets\n",
    "    num_pars = len(list(i2score))\n",
    "    top_k = int(top_k_percent * num_pars)\n",
    "    if top_k >= num_k:\n",
    "        top_k_scores = sorted(i2score.items(), key=itemgetter(1), reverse=True)[int(num_pars*drop_top):int(num_pars*drop_top) + num_k]\n",
    "    else:\n",
    "        top_k_scores = sorted(i2score.items(), key=itemgetter(1), reverse=True)[int(num_pars*drop_top):int(num_pars*drop_top) + top_k]\n",
    "    \n",
    "    i2score = sorted(i2score.items(), key=itemgetter(1), reverse=True)\n",
    "    return i2score, top_k_scores, keep_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_paragraph_len = 20\n",
    "max_paragraph_len = 1000000000000\n",
    "top_k_percent = 0.9\n",
    "num_k = 10000\n",
    "drop_top = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PoorFolk',\n",
       " 'AnnaKarenina',\n",
       " 'DeadSouls',\n",
       " 'TheIdiot',\n",
       " 'CrimeAndPunishment',\n",
       " 'NotesFromUnderground',\n",
       " 'FathersAndSons',\n",
       " 'TheBrothersKaramazov',\n",
       " 'Demons']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(aligned_paragraph_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format: label->translator, x->source+translation\n",
    "# dict: key-> translator, value: list of {source:s, translation:t, index:i}\n",
    "translator_to_pars = {}\n",
    "translator_to_pars_holdout = {}\n",
    "\n",
    "# Holdout\n",
    "# NotesFromUnderground - Katz, PV, Garnett, Hogarth\n",
    "# PoorFolk - McDuff, Hogarth, Garnett\n",
    "# TheIdiot - Garnett, McDuff, PV\n",
    "# CrimeAndPunishment - Katz, McDuff, PV, Garnett\n",
    "holdout_books = ['TheIdiot', 'NotesFromUnderground']\n",
    "ignore_books = []\n",
    "translator_to_pars = {}\n",
    "translator_to_pars_holdout = {}\n",
    "\n",
    "# for each book in train:\n",
    "for book in sorted(list(aligned_paragraph_dataset.keys())):\n",
    "    # get par list of aligned sentences, best k alignments\n",
    "    book_par_list = [list(aligned_paragraph_dataset[book][p].values()) for p in range(len(aligned_paragraph_dataset[book]))]\n",
    "    source_par_list = source_paragraph_dataset[book]\n",
    "\n",
    "    if book in holdout_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, 1.0, 5000, 0, 'sim', min_paragraph_len, max_paragraph_len)\n",
    "    elif book not in ignore_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, top_k_percent, num_k, drop_top, 'sim', min_paragraph_len, max_paragraph_len)\n",
    "    else:\n",
    "        top_k = []\n",
    "\n",
    "    for i, sim in top_k:\n",
    "        par_trans_dict = aligned_paragraph_dataset[book][i]\n",
    "        par_source = source_paragraph_dataset[book][i]\n",
    "\n",
    "        for translator, t in par_trans_dict.items():\n",
    "            t = t.replace('\\\\\\'', '\\'')\n",
    "            datum_dict = {'source':par_source, 'translation': t, 'idx': i, 'book': book, 'sim': sim, 'translator': translator}\n",
    "\n",
    "            if translator not in translator_to_pars.keys():\n",
    "                translator_to_pars[translator] = []\n",
    "                translator_to_pars_holdout[translator] = []\n",
    "                \n",
    "            if book in holdout_books:\n",
    "                translator_to_pars_holdout[translator].append(datum_dict)\n",
    "                # print('len par_list: ', len(book_par_list))\n",
    "                # print('len top_k: ', len(top_k))\n",
    "            else:\n",
    "                translator_to_pars[translator].append(datum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD TRAIN SET\n",
    "\n",
    "# translators = ['Hogarth', 'PV', 'Katz', 'McDuff', 'Garnett']\n",
    "# holdout_books = ['TheIdiot', 'NotesFromUnderground']\n",
    "\n",
    "# translator_to_pars = {}\n",
    "\n",
    "# for t in translators:\n",
    "#     t_pars = []\n",
    "#     for book in list(aligned_paragraph_dataset.keys()):\n",
    "#         if book not in holdout_books:\n",
    "#             if t in aligned_paragraph_dataset[book][0].keys():\n",
    "#                 for par_dict in aligned_paragraph_dataset[book]:\n",
    "#                     t_pars.append(par_dict[t])\n",
    "\n",
    "#     print(t)\n",
    "#     print(len(t_pars))\n",
    "\n",
    "#     i2score, top_k, keep_idx = get_best_alignments(t_pars, top_k_percent, num_k, drop_top, 'sim', min_paragraph_len, max_paragraph_len)\n",
    "    \n",
    "#     # for i, sim in top_k:\n",
    "#     print(len(top_k))\n",
    "#     print()\n",
    "\n",
    "# translator_to_pars_holdout = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'Из лицея молодой человек в первые два года приезжал на вакацию. Во время поездки в Петербург Варвары Петровны и Степана Трофимовича он присутствовал иногда на литературных вечерах, бывавших у мамаши, слушал и наблюдал. Говорил мало и всё по-прежнему был тих и застенчив. К Степану Трофимовичу относился с прежним нежным вниманием, но уже как-то сдержаннее: о высоких предметах и о воспоминаниях прошлого видимо удалялся с ним заговаривать. Кончив курс, он, по желанию мамаши, поступил в военную службу и вскоре был зачислен в один из самых видных гвардейских кавалерийских полков. Показаться мамаше в мундире он не приехал и редко стал писать из Петербурга. Денег Варвара Петровна посылала ему не жалея, несмотря на то что после реформы доход с ее имений упал до того, что в первое время она и половины прежнего дохода не получала. У ней, впрочем, накоплен был долгою экономией некоторый, не совсем маленький капитал. Ее очень интересовали успехи сына в высшем петербургском обществе. Что не удалось ей, то удалось молодому офицеру, богатому и с надеждами. Он возобновил такие знакомства, о которых она и мечтать уже не могла, и везде был принят с большим удовольствием. Но очень скоро начали доходить к Варваре Петровне довольно странные слухи: молодой человек как-то безумно и вдруг закутил. Не то чтоб он играл или очень пил; рассказывали только о какой-то дикой разнузданности, о задавленных рысаками людях, о зверском поступке с одною дамой хорошего общества, с которою он был в связи, а потом оскорбил ее публично. Что-то даже слишком уж откровенно грязное было в этом деле. Прибавляли сверх того, что он какой-то бретер, привязывается и оскорбляет из удовольствия оскорбить. Варвара Петровна волновалась и тосковала. Степан Трофимович уверял ее, что это только первые, буйные порывы слишком богатой организации, что море уляжется и что всё это похоже на юность принца Гарри, кутившего с Фальстафом, Пойнсом и мистрис Квикли, описанную у Шекспира. Варвара Петровна на этот раз не крикнула: «Вздор, вздор!», как повадилась в последнее время покрикивать очень часто на Степана Трофимовича, а, напротив, очень прислушалась, велела растолковать себе подробнее, сама взяла Шекспира и с чрезвычайным вниманием прочла бессмертную хронику. Но хроника ее не успокоила, да и сходства она не так много нашла. Она лихорадочно ждала ответов на несколько своих писем. Ответы не замедлили; скоро было получено роковое известие, что принц Гарри имел почти разом две дуэли, кругом был виноват в обеих, убил одного из своих противников наповал, а другого искалечил и вследствие таковых деяний был отдан под суд. Дело кончилось разжалованием в солдаты, с лишением прав и ссылкой на службу в один из пехотных армейских полков, да и то еще по особенной милости.',\n",
       "  'translation': 'For the first two years the young man came home from the lycée for vacations. While Varvara Petrovna and Stepan Trofimovich were in Petersburg, he was sometimes present at his mother\\'s literary evenings, listening and observing. He spoke little, and was quiet and shy as before. He treated Stepan Trofimovich with the former tender attentiveness, but now somehow more reservedly: he obviously refrained from talking with him about lofty subjects or memories of the past. In accordance with his mama\\'s wish, after completing his studies he entered military service and was soon enrolled in one of the most distinguished regiments of the Horse Guard. He did not come to show himself to his mama in his uniform and now rarely wrote from Petersburg. Varvara Petrovna sent him money without stint, in spite of the fact that the income from her estates fell so much after the reform that at first she did not get even half of her former income. However, through long economy she had saved up a certain not exactly small sum. She was very interested in her son\\'s successes in Petersburg high society. The young officer, rich and with expectations, succeeded where she had not. He renewed acquaintances of which she could no longer even dream, and was received everywhere with great pleasure. But very soon rather strange rumors began to reach Varvara Petrovna: the young man, somehow madly and suddenly, started leading a wild life. Not that he gambled or drank too much; there was only talk of some savage unbridledness, of some people being run over by horses, of some beastly behavior towards a lady of good society with whom he had had a liaison and whom he afterwards publicly insulted. There was something even too frankly dirty about this affair. It was added, furthermore, that he was some sort of swashbuckler, that he picked on people and insulted them for the pleasure of it. Varvara Petrovna was worried and anguished. Stepan Trofimovich assured her that these were merely the first stormy impulses of an overabundant constitution, that the sea would grow calm, and that it all resembled Shakespeare\\'s description of the youth of Prince Harry, carousing with Falstaff, Poins, and Mistress Quickly. This time Varvara Petrovna did not shout \"Nonsense, nonsense!\" as it had lately become her habit to shout quite often at Stepan Trofimovich, but, on the contrary, paid great heed to him, asked him to explain in more detail, herself took Shakespeare and read the immortal chronicle with extreme attention. But the chronicle did not calm her down, nor did  she find all that much resemblance. She waited feverishly for answers to certain of her letters. The answers were not slow in coming; soon the fatal news was  received that Prince Harry had almost simultaneously fought two duels, was entirely to blame for both of them, had killed one of his opponents on the spot and crippled the other, and as a consequence of such deeds had been brought to trial. The affair ended with his being broken to the ranks, stripped of his rights, and exiled to service in one of the infantry regiments, and even that only by special favor.',\n",
       "  'idx': 91,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9765744209289551,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Вещь короткая; даже, если хотите, по-настоящему это и не анекдот, – посыпался бисер. – Впрочем, романист от безделья мог бы испечь роман. Довольно интересная вещица, Прасковья Ивановна, и я уверен, что Лизавета Николаевна с любопытством выслушает, потому что тут много если не чудных, то причудливых вещей. Лет пять тому, в Петербурге, Николай Всеволодович узнал этого господина, – вот этого самого господина Лебядкина, который стоит разиня рот и, кажется, собирался сейчас улизнуть. Извините, Варвара Петровна. Я вам, впрочем, не советую улепетывать, господин отставной чиновник бывшего провиантского ведомства (видите, я отлично вас помню). И мне и Николаю Всеволодовичу слишком известны ваши здешние проделки, в которых, не забудьте это, вы должны будете дать отчет. Еще раз прошу извинения, Варвара Петровна. Николай Всеволодович называл тогда этого господина своим Фальстафом; это, должно быть (пояснил он вдруг), какой-нибудь бывший характер, burlesque,[105] над которым все смеются и который сам позволяет над собою всем смеяться, лишь бы платили деньги. Николай Всеволодович вел тогда в Петербурге жизнь, так сказать, насмешливую, – другим словом не могу определить ее, потому что в разочарование этот человек не впадет, а делом он и сам тогда пренебрегал заниматься. Я говорю про одно лишь тогдашнее время, Варвара Петровна. У Лебядкина этого была сестра, – вот эта самая, что сейчас здесь сидела. Братец и сестрица не имели своего угла и скитались по чужим. Он бродил под арками Гостиного двора, непременно в бывшем мундире, и останавливал прохожих с виду почище, а что наберет – пропивал. Сестрица же кормилась как птица небесная. Она там в углах помогала и за нужду прислуживала. Содом был ужаснейший; я миную картину этой угловой жизни, – жизни, которой из чудачества предавался тогда и Николай Всеволодович. Я только про тогдашнее время, Варвара Петровна; а что касается до «чудачества», то это его собственное выражение. Он многое от меня не скрывает. Mademoiselle Лебядкина, которой одно время слишком часто пришлось встречать Николая Всеволодовича, была поражена его наружностью. Это был, так сказать, бриллиант на грязном фоне ее жизни. Я плохой описатель чувств, а потому пройду мимо; но ее тотчас же подняли дрянные людишки на смех, и она загрустила. Там вообще над нею смеялись, но прежде она вовсе не замечала того. Голова ее уже и тогда была не в порядке, но тогда все-таки не так, как теперь. Есть основание предположить, что в детстве, через какую-то благодетельницу, она чуть было не получила воспитания. Николай Всеволодович никогда не обращал на нее ни малейшего внимания и играл больше в старые замасленные карты по четверть копейки в преферанс с чиновниками. Но раз, когда ее обижали, он (не спрашивая причины) схватил одного чиновника за шиворот и спустил изо второго этажа в окно. Никаких рыцарских негодований в пользу оскорбленной невинности тут не было; вся операция произошла при общем смехе, и смеялся всех больше Николай Всеволодович сам; когда же всё кончилось благополучно, то помирились и стали пить пунш. Но угнетенная невинность сама про то не забыла. Разумеется, кончилось окончательным сотрясением ее умственных способностей. Повторяю, я плохой описатель чувств, но тут главное мечта. А Николай Всеволодович, как нарочно, еще более раздражал мечту: вместо того чтобы рассмеяться, он вдруг стал обращаться к mademoiselle Лебядкиной с неожиданным уважением. Кириллов, тут бывший (чрезвычайный оригинал, Варвара Петровна, и чрезвычайно отрывистый человек; вы, может быть, когда-нибудь его увидите, он теперь здесь), ну так вот, этот Кириллов, который, по обыкновению, всё молчит, а тут вдруг разгорячился, заметил, я помню, Николаю Всеволодовичу, что тот третирует эту госпожу как маркизу и тем окончательно ее добивает. Прибавлю, что Николай Всеволодович несколько уважал этого Кириллова. Что же, вы думаете, он ему ответил: «Вы полагаете, господин Кириллов, что я смеюсь над нею; разуверьтесь, я в самом деле ее уважаю, потому что она всех нас лучше». И, знаете, таким серьезным тоном сказал. Между тем в эти два-три месяца он, кроме «здравствуйте» да «прощайте», в сущности, не проговорил с ней ни слова. Я, тут бывший, наверно помню, что она до того уже, наконец, дошла, что считала его чем-то вроде жениха своего, не смеющего ее «похитить» единственно потому, что у него много врагов и семейных препятствий или что-то в этом роде. Много тут было смеху! Кончилось тем, что когда Николаю Всеволодовичу пришлось тогда отправляться сюда, он, уезжая, распорядился о ее содержании и, кажется, довольно значительном ежегодном пенсионе, рублей в триста по крайней мере, если не более. Одним словом, положим, всё это с его стороны баловство, фантазия преждевременно уставшего человека, – пусть даже, наконец, как говорил Кириллов, это был новый этюд пресыщенного человека с целью узнать, до чего можно довести сумасшедшую калеку. «Вы, говорит, нарочно выбрали самое последнее существо, калеку, покрытую вечным позором и побоями, – и вдобавок зная, что это существо умирает к вам от комической любви своей, – и вдруг вы нарочно принимаетесь ее морочить, единственно для того, чтобы посмотреть, что из этого выйдет!» Чем, наконец, так особенно виноват человек в фантазиях сумасшедшей женщины, с которой, заметьте, он вряд ли две фразы во всё время выговорил! Есть вещи, Варвара Петровна, о которых не только нельзя умно говорить, но о которых и начинать-то говорить неумно. Ну пусть, наконец, чудачество – но ведь более-то уж ничего нельзя сказать; а между тем теперь вот из этого сделали историю… Мне отчасти известно, Варвара Петровна, о том, что здесь происходит. Рассказчик вдруг оборвал и повернулся было к Лебядкину, но Варвара Петровна остановила его; она была в сильнейшей экзальтации.',\n",
       "  'translation': '\"It\\'s a short matter; in fact, if you like, it\\'s not even an anecdote,\" the beads began spilling out. \"However, a novelist might cook up a novel from it in an idle moment. It\\'s quite an interesting little matter, Praskovya Ivanovna, and I\\'m sure Lizaveta Nikolaevna will listen with curiosity, because there are many things here which, if not queer, are at least quaint. About five years ago, in Petersburg, Nikolai Vsevolodovich got to know this gentleman-this same Mr. Lebyadkin who is standing here with his mouth hanging open and, it seems, was just about to slip away. Forgive me, Varvara Petrovna. Incidentally, I\\'d advise you not to take to your heels, mister retired official of the former supply department (you see, I remember you perfectly). Both I and Nikolai Vsevolodovich are all too well informed of  your local tricks, of which, don\\'t forget, you will have to give an accounting. Once again I ask your forgiveness, Varvara Petrovna. Nikolai Vsevolodovich used to call  this gentleman his Falstaff -that must be some former character,\" he suddenly explained, \"some burlesque everyone laughs at and who allows everyone to laugh at him, so long as they pay money. The life Nikolai Vsevolodovich then led in Petersburg was, so to speak, a jeering one-I cannot define it by any other word, because he was not a man to fall into disillusionment, and he scorned then to do anything serious. I\\'m talking only about that time, Varvara Petrovna. This Lebyadkin had a sister-the very one who was just sitting here. This nice brother and sister had no corner of their own, and wandered about staying with various people. He loitered under the arcades of the Gostiny Dvor, unfailingly wearing his former uniform, and stopped the cleanerlooking passersby, and whatever he collected he would spend on drink. His sister lived like the birds of the air. She helped out in those corners and served in exchange for necessities. It was a most terrible Sodom; I\\'ll pass over the picture of this corner life-the life to which Nikolai Vsevolodovich then gave himself out of whimsicality. This was only then, Varvara Petrovna; and as for \\'whimsicality,\\' the expression is his. There is much that he does not conceal from me. Mademoiselle Lebyadkin, who at a certain period happened to run into Nikolai Vsevolodovich all too often, was struck by his appearance. He was, so to speak, a diamond set against the dirty background of her life. I\\'m a poor describer of feelings, so I\\'ll pass that over; but rotten little people immediately made fun of her, and she grew sad. They generally laughed at her there, but before she didn\\'t notice it. She was already not right in the head then, but less so than now. There\\'s reason to think that in childhood, through some benefactress, she almost received an education. Nikolai Vsevolodovich never paid the slightest attention to her, and rather spent his time playing old greasy cards, the game of preference for quarterkopeck stakes, with some clerks. But once when she was being mistreated, he, without asking why, grabbed one clerk by the scruff of the neck and chucked him out the secondstory window. There wasn\\'t any chivalrous indignation in favor of offended innocence in it; the whole operation took place amid general laughter, and Nikolai Vsevolodovich himself laughed most of all; everything eventually came to a good end, they made peace and began drinking punch. But oppressed innocence herself did not forget it. Of course, it ended with the final shaking of her mental faculties. I repeat, I\\'m a poor describer of feelings, but the main thing here was the dream. And Nikolai Vsevolodovich, as if on purpose, aroused the dream even more; instead of just laughing at it, he suddenly began addressing Mademoiselle Lebyadkin with unexpected esteem. Kirillov, who was there (an exceedingly original man, Varvara Petrovna, and an exceedingly abrupt one; perhaps you\\'ll meet him one day, he\\'s here now), well, so this Kirillov, who ordinarily is always silent, but then suddenly got excited, observed to Nikolai Vsevolodovich, as I remember, that his treating this lady as a marquise was finally going to finish her off. I will add that Nikolai Vsevolodovich had a certain respect for this Kirillov. And how do you think he answered him? \\'You assume, Mr. Kirillov, that I am laughing at her; let me assure you that I do indeed respect her, because she is better than any of us.\\' And, you know, he said it in such a serious tone. Though, in fact, during those two or three months he hadn\\'t said a word to her except \\'hello\\' and \\'goodbye.\\' I, who was there, remember for a certainty that she finally reached the point of regarding him as something like her fiancé, who did not dare to \\'abduct\\' her solely because he had many enemies and family obstacles, or something of the sort. There was much laughter over that! In the end, when Nikolai Vsevolodovich had to come here that time, as he was leaving he arranged for her keep, and it seems it was quite a substantial yearly pension, at least three hundred roubles, if not more. In short, let\\'s say it was all selfindulgence, the fancy of a prematurely weary man-let it be, finally, as Kirillov was saying, a new étude by ajaded man, with the object of finding out what a mad cripple can be brought to. \\'You chose on purpose,\\' he said, \\'the very least of beings, a cripple covered in eternal shame and beatings-and knowing, besides, that this being is dying of her comical love for you-and you suddenly start to flummox her on purpose, solely to see what will come of it!\\' Why, finally, is a man so especially to blame for the fantasy of a mad woman to whom, notice, he had hardly spoken two sentences during that whole time! There are things, Varvara Petrovna, of which it is not only impossible to speak intelligently, but of which it is not intelligent even to begin speaking. Well, let it be whimsicality, finally- but that\\'s all one can say; and yet quite a story has been made of it now... I\\'m partly informed, Varvara Petrovna, of what is going on here.\" The narrator suddenly broke off and was turning to Lebyadkin, but Varvara Petrovna stopped him; she was in the greatest exaltation.',\n",
       "  'idx': 1263,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9765353202819824,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Да кто? Кто велел вам сюда приходить? – допрашивала Варвара Петровна.',\n",
       "  'translation': '\"But, who? Who told you to come here?\" Varvara Petrovna questioned.',\n",
       "  'idx': 1228,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9762787818908691,\n",
       "  'translator': 'PV'},\n",
       " {'source': 'Так называемое у нас имение Степана Трофимовича (душ пятьдесят по старинному счету, и смежное со Скворешниками) было вовсе не его, а принадлежало первой его супруге, а стало быть, теперь их сыну, Петру Степановичу Верховенскому. Степан Трофимович только опекунствовал, а потому, когда птенец оперился, действовал по формальной от него доверенности на управление имением. Сделка для молодого человека была выгодная: он получал с отца в год до тысячи рублей в виде дохода с имения, тогда как оно при новых порядках не давало и пятисот (а может быть, и того менее). Бог знает как установились подобные отношения. Впрочем, всю эту тысячу целиком высылала Варвара Петровна, а Степан Трофимович ни единым рублем в ней не участвовал. Напротив, весь доход с землицы оставлял у себя в кармане и, кроме того, разорил ее вконец, сдав ее в аренду какому-то промышленнику и, тихонько от Варвары Петровны, продав на сруб рощу, то есть главную ее ценность. Эту рощицу он уже давно продавал урывками. Вся она стоила по крайней мере тысяч восемь, а он взял за нее только пять. Но он иногда слишком много проигрывал в клубе, а просить у Варвары Петровны боялся. Она скрежетала зубами, когда наконец обо всем узнала. И вдруг теперь сынок извещал, что приедет сам продать свои владения во что бы ни стало, а отцу поручал неотлагательно позаботиться о продаже. Ясное дело, что при благородстве и бескорыстии Степана Трофимовича ему стало совестно пред се cher enfant[41] (которого он в последний раз видел целых девять лет тому назад, в Петербурге, студентом). Первоначально все имение могло стоить тысяч тринадцать или четырнадцать, теперь вряд ли кто бы дал за него и пять. Без сомнения, Степан Трофимович имел полное право, по смыслу формальной доверенности, продать лес и, поставив в счет тысячерублевый невозможный ежегодный доход, столько лет высылавшийся аккуратно, сильно оградить себя при расчете. Но Степан Трофимович был благороден, со стремлениями высшими. В голове его мелькнула одна удивительно красивая мысль: когда приедет Петруша, вдруг благородно выложить на стол самый высший maximum цены, то есть даже пятнадцать тысяч, без малейшего намека на высылавшиеся до сих пор суммы, и крепко-крепко, со слезами, прижать к груди се cher fils,[42] чем и покончить все счеты. Отдаленно и осторожно начал он развертывать эту картинку пред Варварой Петровной. Он намекал, что это даже придаст какой-то особый, благородный оттенок их дружеской связи… их «идее». Это выставило бы в таком бескорыстном и великодушном виде прежних отцов и вообще прежних людей сравнительно с новою легкомысленною и социальною молодежью. Много еще он говорил, но Варвара Петровна всё отмалчивалась. Наконец сухо объявила ему, что согласна купить их землю и даст за нее maximum цены, то есть тысяч шесть, семь (и за четыре можно было купить). Об остальных же восьми тысячах, улетевших с рощей, не сказала ни слова.',\n",
       "  'translation': 'Stepan Trofimovich\\'s estate, as we used to call it (about fifty souls by the old way of reckoning, and adjoining Skvoreshniki), was not his at all, but had belonged to his first wife, and so now to their son, Pyotr Stepanovich Verkhovensky. Stepan Trofimovich was merely the trustee, and thus, once the nestling was fully fledged, acted through a formal warrant as manager of the estate. For the young man it was a profitable deal: he received up to a thousand roubles a year from his father as income from the estate, while under the new regulations it did not yield as much as five hundred (and perhaps even less). God knows how such arrangements were set up. However, the entire thousand was sent by Varvara Petrovna, and Stepan Trofimovich did not contribute a single rouble to it. On the contrary, he pocketed all the income from this bit of land, and, furthermore, ruined it altogether by leasing it to some dealer and, in secret from Varvara Petrovna, selling the timber that was its main valuable asset. He had been selling this timber piecemeal for a long time. Its total worth was about eight thousand at least, yet he got only five for it. But he sometimes lost too much at the club, and was afraid to ask Varvara Petrovna. She ground her teeth when she finally learned of it all. And now the boy suddenly notified him that he was coming himself to sell his property at all costs, and charged his father with promptly arranging for the sale. It was clear that Stepan Trofimovich, being a lofty and disinterested man, felt ashamed before ce cher enfant (whom he had last seen as a student in Petersburg all of nine years earlier). Originally, the entire estate might have been worth some thirteen or fourteen thousand, but now it was unlikely that anyone would give five for it. Stepan Trofimovich undoubtedly had every right, in terms of the formal warrant, to sell the timber, and taking into account the impossible annual income of a thousand roubles, which had been sent punctually for so many years, could make a good defense of himself in any final settlement. But Stepan Trofimovich was noble and had lofty aspirations. A remarkably beautiful thought flashed in his head: to lay out nobly on the table, when Petrusha came, the highest maximum of the pricethat is, even fifteen thousand-without the slightest hint at the sums that had been sent previously, and then firmly, very firmly, with tears, to press ce cher fils to his heart, and so settle all accounts. He began remotely and cautiously unfolding this picture before Varvara Petrovna. He hinted that it would even add some special, noble tinge to their friendly connection ... to their \"idea.\" It would show former fathers and former people generally in such a disinterested and magnanimous light, as compared with the new frivolous and social youth. He said many other things, but Varvara Petrovna kept silent. At last she dryly informed him that she would agree to buy their land and would pay the maximum price for it-that is, six or seven thousand (even four would have been enough). Of the remaining eight thousand that had flown away with the timber, she did not say a word.',\n",
       "  'idx': 289,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9760987162590027,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Шатов? Это брат Дарьи Павловны…',\n",
       "  'translation': '\"Shatov? He is Darya Pavlovna\\'s brother...\"',\n",
       "  'idx': 528,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9759403467178345,\n",
       "  'translator': 'PV'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlist = sorted(translator_to_pars['PV'], key=lambda d: d['sim'], reverse=True) \n",
    "newlist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547\n"
     ]
    }
   ],
   "source": [
    "min_len = len(translator_to_pars['Hogarth'])\n",
    "print(min_len)\n",
    "for t in translator_to_pars.keys():\n",
    "    keep = sorted(translator_to_pars[t], key=lambda d: d['sim'], reverse=True)[:min_len]\n",
    "    translator_to_pars[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "min_len_h = len(translator_to_pars_holdout['Hogarth'])\n",
    "print(min_len_h)\n",
    "for t in translator_to_pars_holdout.keys():\n",
    "    keep = sample(translator_to_pars_holdout[t], min_len_h) \n",
    "    translator_to_pars_holdout[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Garnett': '\"What?\" Pyotr Stepanovitch pricked up his ears. \"What idea? Did he tell you something himself?\"',\n",
       " 'PV': '\"What?\" Pyotr Stepanovich pricked up his ears. \"What idea? Did he tell you something himself?\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_paragraph_dataset['Demons'][4748]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All\n",
      "PV 3980\n",
      "Garnett 3980\n",
      "Katz 3980\n",
      "McDuff 3980\n",
      "Hogarth 3980\n",
      "Total 19900\n",
      "\n",
      "Train\n",
      "PV 3547\n",
      "Garnett 3547\n",
      "Katz 3547\n",
      "McDuff 3547\n",
      "Hogarth 3547\n",
      "\n",
      "Holdout\n",
      "PV 433\n",
      "Garnett 433\n",
      "Katz 433\n",
      "McDuff 433\n",
      "Hogarth 433\n",
      "Train total:  17735\n",
      "Val/Test total:  2165\n",
      "train % =  0.8912060301507537\n",
      "holdout % =  0.10879396984924623\n"
     ]
    }
   ],
   "source": [
    "abs_total = 0\n",
    "print('\\nAll')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    both = len(translator_to_pars_holdout[k]) + len(translator_to_pars[k])\n",
    "    print(k, both)\n",
    "    abs_total += both\n",
    "print('Total', abs_total)\n",
    "\n",
    "train_total = 0\n",
    "min_class = 100000000000\n",
    "print('\\nTrain')\n",
    "for k in translator_to_pars.keys():\n",
    "    print(k, len(translator_to_pars[k]))\n",
    "    if len(translator_to_pars[k]) < min_class:\n",
    "        min_class = len(translator_to_pars[k])\n",
    "    \n",
    "train_total = len(translator_to_pars.keys()) * min_class\n",
    "\n",
    "holdout_total = 0\n",
    "min_class_h = 100000000000\n",
    "print('\\nHoldout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    print(k, len(translator_to_pars_holdout[k]))\n",
    "    if len(translator_to_pars_holdout[k]) < min_class_h:\n",
    "        min_class_h = len(translator_to_pars_holdout[k])\n",
    "\n",
    "holdout_total = len(translator_to_pars.keys()) * min_class_h\n",
    "\n",
    "print('Train total: ', min_class*5)\n",
    "print('Val/Test total: ', min_class_h*5)\n",
    "print()\n",
    "print('train % = ', train_total/(holdout_total+train_total))\n",
    "print('holdout % = ', holdout_total/(holdout_total+train_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 4 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(list(translator_to_pars.keys()))\n",
    "print(le.transform([\"Garnett\", \"McDuff\", \"PV\", \"Katz\", \"Hogarth\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'idx': 91, 'book': 'Demons', 'labels': 4, 'concat': 'Из лицея молодой человек в первые два года приезжал на вакацию. Во время поездки в Петербург Варвары Петровны и Степана Трофимовича он присутствовал иногда на литературных вечерах, бывавших у мамаши, слушал и наблюдал. Говорил мало и всё по-прежнему был тих и застенчив. К Степану Трофимовичу относился с прежним нежным вниманием, но уже как-то сдержаннее: о высоких предметах и о воспоминаниях прошлого видимо удалялся с ним заговаривать. Кончив курс, он, по желанию мамаши, поступил в военную службу и вскоре был зачислен в один из самых видных гвардейских кавалерийских полков. Показаться мамаше в мундире он не приехал и редко стал писать из Петербурга. Денег Варвара Петровна посылала ему не жалея, несмотря на то что после реформы доход с ее имений упал до того, что в первое время она и половины прежнего дохода не получала. У ней, впрочем, накоплен был долгою экономией некоторый, не совсем маленький капитал. Ее очень интересовали успехи сына в высшем петербургском обществе. Что не удалось ей, то удалось молодому офицеру, богатому и с надеждами. Он возобновил такие знакомства, о которых она и мечтать уже не могла, и везде был принят с большим удовольствием. Но очень скоро начали доходить к Варваре Петровне довольно странные слухи: молодой человек как-то безумно и вдруг закутил. Не то чтоб он играл или очень пил; рассказывали только о какой-то дикой разнузданности, о задавленных рысаками людях, о зверском поступке с одною дамой хорошего общества, с которою он был в связи, а потом оскорбил ее публично. Что-то даже слишком уж откровенно грязное было в этом деле. Прибавляли сверх того, что он какой-то бретер, привязывается и оскорбляет из удовольствия оскорбить. Варвара Петровна волновалась и тосковала. Степан Трофимович уверял ее, что это только первые, буйные порывы слишком богатой организации, что море уляжется и что всё это похоже на юность принца Гарри, кутившего с Фальстафом, Пойнсом и мистрис Квикли, описанную у Шекспира. Варвара Петровна на этот раз не крикнула: «Вздор, вздор!», как повадилась в последнее время покрикивать очень часто на Степана Трофимовича, а, напротив, очень прислушалась, велела растолковать себе подробнее, сама взяла Шекспира и с чрезвычайным вниманием прочла бессмертную хронику. Но хроника ее не успокоила, да и сходства она не так много нашла. Она лихорадочно ждала ответов на несколько своих писем. Ответы не замедлили; скоро было получено роковое известие, что принц Гарри имел почти разом две дуэли, кругом был виноват в обеих, убил одного из своих противников наповал, а другого искалечил и вследствие таковых деяний был отдан под суд. Дело кончилось разжалованием в солдаты, с лишением прав и ссылкой на службу в один из пехотных армейских полков, да и то еще по особенной милости. <SEP> For the first two years the young man came home from the lycée for vacations. While Varvara Petrovna and Stepan Trofimovich were in Petersburg, he was sometimes present at his mother\\'s literary evenings, listening and observing. He spoke little, and was quiet and shy as before. He treated Stepan Trofimovich with the former tender attentiveness, but now somehow more reservedly: he obviously refrained from talking with him about lofty subjects or memories of the past. In accordance with his mama\\'s wish, after completing his studies he entered military service and was soon enrolled in one of the most distinguished regiments of the Horse Guard. He did not come to show himself to his mama in his uniform and now rarely wrote from Petersburg. Varvara Petrovna sent him money without stint, in spite of the fact that the income from her estates fell so much after the reform that at first she did not get even half of her former income. However, through long economy she had saved up a certain not exactly small sum. She was very interested in her son\\'s successes in Petersburg high society. The young officer, rich and with expectations, succeeded where she had not. He renewed acquaintances of which she could no longer even dream, and was received everywhere with great pleasure. But very soon rather strange rumors began to reach Varvara Petrovna: the young man, somehow madly and suddenly, started leading a wild life. Not that he gambled or drank too much; there was only talk of some savage unbridledness, of some people being run over by horses, of some beastly behavior towards a lady of good society with whom he had had a liaison and whom he afterwards publicly insulted. There was something even too frankly dirty about this affair. It was added, furthermore, that he was some sort of swashbuckler, that he picked on people and insulted them for the pleasure of it. Varvara Petrovna was worried and anguished. Stepan Trofimovich assured her that these were merely the first stormy impulses of an overabundant constitution, that the sea would grow calm, and that it all resembled Shakespeare\\'s description of the youth of Prince Harry, carousing with Falstaff, Poins, and Mistress Quickly. This time Varvara Petrovna did not shout \"Nonsense, nonsense!\" as it had lately become her habit to shout quite often at Stepan Trofimovich, but, on the contrary, paid great heed to him, asked him to explain in more detail, herself took Shakespeare and read the immortal chronicle with extreme attention. But the chronicle did not calm her down, nor did  she find all that much resemblance. She waited feverishly for answers to certain of her letters. The answers were not slow in coming; soon the fatal news was  received that Prince Harry had almost simultaneously fought two duels, was entirely to blame for both of them, had killed one of his opponents on the spot and crippled the other, and as a consequence of such deeds had been brought to trial. The affair ended with his being broken to the ranks, stripped of his rights, and exiled to service in one of the infantry regiments, and even that only by special favor.', 'translator': 'PV', 'sim': 0.9765744209289551, 'src': 'Из лицея молодой человек в первые два года приезжал на вакацию. Во время поездки в Петербург Варвары Петровны и Степана Трофимовича он присутствовал иногда на литературных вечерах, бывавших у мамаши, слушал и наблюдал. Говорил мало и всё по-прежнему был тих и застенчив. К Степану Трофимовичу относился с прежним нежным вниманием, но уже как-то сдержаннее: о высоких предметах и о воспоминаниях прошлого видимо удалялся с ним заговаривать. Кончив курс, он, по желанию мамаши, поступил в военную службу и вскоре был зачислен в один из самых видных гвардейских кавалерийских полков. Показаться мамаше в мундире он не приехал и редко стал писать из Петербурга. Денег Варвара Петровна посылала ему не жалея, несмотря на то что после реформы доход с ее имений упал до того, что в первое время она и половины прежнего дохода не получала. У ней, впрочем, накоплен был долгою экономией некоторый, не совсем маленький капитал. Ее очень интересовали успехи сына в высшем петербургском обществе. Что не удалось ей, то удалось молодому офицеру, богатому и с надеждами. Он возобновил такие знакомства, о которых она и мечтать уже не могла, и везде был принят с большим удовольствием. Но очень скоро начали доходить к Варваре Петровне довольно странные слухи: молодой человек как-то безумно и вдруг закутил. Не то чтоб он играл или очень пил; рассказывали только о какой-то дикой разнузданности, о задавленных рысаками людях, о зверском поступке с одною дамой хорошего общества, с которою он был в связи, а потом оскорбил ее публично. Что-то даже слишком уж откровенно грязное было в этом деле. Прибавляли сверх того, что он какой-то бретер, привязывается и оскорбляет из удовольствия оскорбить. Варвара Петровна волновалась и тосковала. Степан Трофимович уверял ее, что это только первые, буйные порывы слишком богатой организации, что море уляжется и что всё это похоже на юность принца Гарри, кутившего с Фальстафом, Пойнсом и мистрис Квикли, описанную у Шекспира. Варвара Петровна на этот раз не крикнула: «Вздор, вздор!», как повадилась в последнее время покрикивать очень часто на Степана Трофимовича, а, напротив, очень прислушалась, велела растолковать себе подробнее, сама взяла Шекспира и с чрезвычайным вниманием прочла бессмертную хронику. Но хроника ее не успокоила, да и сходства она не так много нашла. Она лихорадочно ждала ответов на несколько своих писем. Ответы не замедлили; скоро было получено роковое известие, что принц Гарри имел почти разом две дуэли, кругом был виноват в обеих, убил одного из своих противников наповал, а другого искалечил и вследствие таковых деяний был отдан под суд. Дело кончилось разжалованием в солдаты, с лишением прав и ссылкой на службу в один из пехотных армейских полков, да и то еще по особенной милости.', 'tgt': 'For the first two years the young man came home from the lycée for vacations. While Varvara Petrovna and Stepan Trofimovich were in Petersburg, he was sometimes present at his mother\\'s literary evenings, listening and observing. He spoke little, and was quiet and shy as before. He treated Stepan Trofimovich with the former tender attentiveness, but now somehow more reservedly: he obviously refrained from talking with him about lofty subjects or memories of the past. In accordance with his mama\\'s wish, after completing his studies he entered military service and was soon enrolled in one of the most distinguished regiments of the Horse Guard. He did not come to show himself to his mama in his uniform and now rarely wrote from Petersburg. Varvara Petrovna sent him money without stint, in spite of the fact that the income from her estates fell so much after the reform that at first she did not get even half of her former income. However, through long economy she had saved up a certain not exactly small sum. She was very interested in her son\\'s successes in Petersburg high society. The young officer, rich and with expectations, succeeded where she had not. He renewed acquaintances of which she could no longer even dream, and was received everywhere with great pleasure. But very soon rather strange rumors began to reach Varvara Petrovna: the young man, somehow madly and suddenly, started leading a wild life. Not that he gambled or drank too much; there was only talk of some savage unbridledness, of some people being run over by horses, of some beastly behavior towards a lady of good society with whom he had had a liaison and whom he afterwards publicly insulted. There was something even too frankly dirty about this affair. It was added, furthermore, that he was some sort of swashbuckler, that he picked on people and insulted them for the pleasure of it. Varvara Petrovna was worried and anguished. Stepan Trofimovich assured her that these were merely the first stormy impulses of an overabundant constitution, that the sea would grow calm, and that it all resembled Shakespeare\\'s description of the youth of Prince Harry, carousing with Falstaff, Poins, and Mistress Quickly. This time Varvara Petrovna did not shout \"Nonsense, nonsense!\" as it had lately become her habit to shout quite often at Stepan Trofimovich, but, on the contrary, paid great heed to him, asked him to explain in more detail, herself took Shakespeare and read the immortal chronicle with extreme attention. But the chronicle did not calm her down, nor did  she find all that much resemblance. She waited feverishly for answers to certain of her letters. The answers were not slow in coming; soon the fatal news was  received that Prince Harry had almost simultaneously fought two duels, was entirely to blame for both of them, had killed one of his opponents on the spot and crippled the other, and as a consequence of such deeds had been brought to trial. The affair ended with his being broken to the ranks, stripped of his rights, and exiled to service in one of the infantry regiments, and even that only by special favor.'}]\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "i = 0\n",
    "for tr in translator_to_pars.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat,  'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list.append(sent_dict)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "data_list_holdout = []\n",
    "i = 0\n",
    "for tr in translator_to_pars_holdout.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars_holdout[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat, 'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list_holdout.append(sent_dict)\n",
    "        i += 1\n",
    "        \n",
    "print(data_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df_holdout = pd.DataFrame(data_list_holdout)\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17735, 8)\n",
      "(2165, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat, 'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "df_holdout_X = df_holdout[['idx','book', 'concat', 'translator', 'sim', 'src', 'tgt']]\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  (17735, 8)\n",
      "val size:  (1083, 8)\n",
      "test size:  (1082, 8)\n",
      "{'label': 4, 'text': \"Когда Левин вошел с Облонским в гостиницу, он не мог не заметить некоторой особенности выражения, как бы сдержанного сияния, на лице и во всей фигуре Степана Аркадьича. Облонский снял пальто и в шляпе набекрень прошел в столовую, отдавая приказания липнувшим к нему татарам во фраках и с салфетками. Кланяясь направо и налево нашедшимся и тут, как везде, радостно встречавшим его знакомым, он подошел к буфету, закусил водку рыбкой и что-то такое сказал раскрашенной, в ленточках, кружевах и завитушках француженке, сидевшей за конторкой, что даже эта француженка искренно засмеялась. Левин же только оттого не выпил водки, что ему оскорбительна была эта француженка, вся составленная, казалось, из чужих волос, poudre de riz и vinaigre de toilette.[44] Он, как от грязного места, поспешно отошел от нее. Вся душа его была переполнена воспоминанием о Кити, и в глазах его светилась улыбка торжества и счастия. <SEP> As Levin entered the hotel with Oblonsky, he could not help noticing a certain special expression, as if of restrained radiance, on the face and in the whole figure of Stepan Arkadyich. Oblonsky took off his coat and with his hat cocked passed into the restaurant, giving orders to the Tartars1¢ in tailcoats who clung to him, napkins over their arms. Bowing right and left to the joyful greetings of acquaintances who turned up there, as everywhere, he went to the bar, followed his glass of vodka with a bit of fish, and said something to the painted Frenchwoman in ribbons, lace and ringlets who was sitting at the counter, so that even this Frenchwoman burst into genuine laughter. Levin did not drink vodka, if only because this Frenchwoman, who seemed to consist entirely of other people's hair, poudre de riz and vinaigre de toilette,» was offensive to him. He hastened to step away from her as from a dirty spot. His whole soul was overflowing with the remembrance of Kitty, and in his eyes shone a smile of triumph and happiness.\"}\n",
      "train size:  17735\n",
      "val size:  1083\n",
      "test size:  1082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_texts = df['sentence'].values.tolist()\n",
    "# train_labels = df['labels'].values.tolist()\n",
    "# test_texts = df_holdout['concat'].values.tolist()\n",
    "# test_labels = df_holdout['labels'].values.tolist()\n",
    "\n",
    "# train_texts =  df['concat'].values.tolist()\n",
    "# train_labels = df['labels'].values.tolist()\n",
    "\n",
    "test_texts, val_texts, test_labels, val_labels = train_test_split(\n",
    "    df_holdout_X, df_holdout['labels'],\n",
    "    stratify = df_holdout['labels'], shuffle=True, test_size=0.5\n",
    ")\n",
    "\n",
    "train_df = df\n",
    "test_df = pd.concat([test_texts, test_labels], axis=1)\n",
    "val_df = pd.concat([val_texts, val_labels], axis=1)\n",
    "print('train size: ', train_df.shape)\n",
    "print('val size: ', val_df.shape)\n",
    "print('test size: ', test_df.shape)\n",
    "\n",
    "sentences = {}\n",
    "sentences['train'] = [{'label': row['labels'], 'text':row['concat']} for i, row in train_df.iterrows()]\n",
    "sentences['test'] = [{'label': row['labels'], 'text':row['concat']} for i, row in test_df.iterrows()]\n",
    "sentences['val'] = [{'label': row['labels'], 'text':row['concat']} for i, row in val_df.iterrows()]\n",
    "# for t, l in zip(train_texts, train_labels):\n",
    "#     datum = {'label': l, 'text': t}\n",
    "#     sentences['train'].append(datum)\n",
    "\n",
    "# for t, l in zip(val_texts, val_labels):\n",
    "#     datum = {'label': l, 'text': t}\n",
    "#     sentences['val'].append(datum)\n",
    "\n",
    "# for t, l in zip(test_texts, test_labels):\n",
    "#     datum = {'label': l, 'text': t}\n",
    "#     sentences['test'].append(datum)\n",
    "\n",
    "print(sentences['train'][770])\n",
    "\n",
    "\n",
    "print('train size: ', len(sentences['train']))\n",
    "print('val size: ', len(sentences['val']))\n",
    "print('test size: ', len(sentences['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/kkatsy/litMT/classification_experiments_dataset.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentences, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/kkatsy/litMT/classification_experiments_dataset.pickle', 'wb') as handle:\n",
    "#     a = pickle.load(sentences, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"bert-base-multilingual-cased\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102, 10882, 62900, 19226, 10105, 19157, 10169, 43019, 57098, 11445, 117, 10261, 12174, 10472, 15217, 75349, 19113, 169, 16620, 14478, 23515, 117, 10146, 12277, 10108, 17333, 32782, 10336, 31031, 16460, 117, 10135, 10105, 13295, 10111, 10106, 10105, 21047, 17527, 10108, 41653, 10206, 67552, 51210, 14401, 119, 43019, 57098, 11445, 12149, 11898, 10226, 63646, 10111, 10169, 10226, 11250, 11170, 72333, 20006, 10708, 10105, 26234, 117, 24426, 31303, 10114, 10105, 14248, 95941, 10107, 10759, 110883, 10106, 48497, 83592, 10806, 10479, 171, 25497, 10114, 10957, 117, 64728, 39460, 10491, 10455, 28150, 119, 74857, 10230, 13448, 10111, 12153, 10114, 10105, 12541, 62026, 10604, 30518, 23203, 18800, 10108, 24742, 42948, 28524, 10479, 21031, 10741, 11155, 117, 10146, 14234, 30935, 117, 10261, 13446, 10114, 10105, 18121, 117, 15689, 10226, 32362, 10108, 12556, 41513, 10169, 169, 17684, 10108, 26228, 117, 10111, 12415, 26133, 10114, 10105, 37992, 11894, 79999, 10106, 29956, 14496, 13326, 117, 23455, 10112, 10111, 21550, 36630, 10479, 10134, 62151, 10160, 10105, 46298, 117, 10380, 10189, 13246, 10531, 11894, 79999, 11499, 21328, 10708, 15331, 63251, 27207, 45953, 119, 62900, 12172, 10472, 69423, 12556, 41513, 117, 12277, 10893, 12373, 10531, 11894, 79999, 117, 10479, 64676, 10114, 67120, 34053, 10108, 10684, 11426, 112, 187, 40830, 117, 23491, 16419, 10104, 29956, 10305, 10111, 38973, 11542, 26095, 10104, 63131, 12131, 117, 220, 10134, 31820, 10114, 10957, 119, 10357, 10393, 52423, 10162, 10114, 31877, 14942, 10188, 10485, 10146, 10188, 169, 15895, 11195, 28504, 119, 11597, 21047, 33068, 10134, 10491, 105739, 10230, 10169, 10105, 11639, 60503, 81371, 10419, 10108, 53623, 117, 10111, 10106, 10226, 38144, 48201, 12926, 102]\n"
     ]
    }
   ],
   "source": [
    "text1, text2 = sentences['train'][770]['text'].split(' <SEP> ')\n",
    "tokenized1 = tokenizer([text1, text2], [text1, text2], padding='max_length', max_length=512, truncation='longest_first')\n",
    "tokenized2 = tokenizer(text1, text2, padding='max_length', max_length=512, truncation='longest_first')\n",
    "\n",
    "\n",
    "print(tokenized2.input_ids[256:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=sentences['train']))\n",
    "val_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=sentences['val']))\n",
    "test_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=sentences['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d1cfe50d6542818883ad8f74b51f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7966ed9b512a4d6aaf3e442d13fb3f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(src, tgt, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest_first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_function)\n\u001b[0;32m----> 6\u001b[0m tokenized_val \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_function)\n\u001b[1;32m      7\u001b[0m tokenized_test \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_function)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:591\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    592\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:556\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    554\u001b[0m }\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    557\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3089\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3083\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3084\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3085\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3087\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3088\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3089\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3090\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3091\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3442\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3440\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3442\u001b[0m     example \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m   3443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3345\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3344\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3345\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[1;32m   3346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3347\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3348\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3349\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[0;34m(datum)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(datum):\n\u001b[1;32m      2\u001b[0m     src, tgt \u001b[38;5;241m=\u001b[39m datum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m <SEP> \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(src, tgt, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest_first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2801\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2802\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2803\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2909\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2890\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2891\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2906\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2907\u001b[0m     )\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2910\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2911\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2912\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2913\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2914\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2915\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2916\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2917\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2918\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2919\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2920\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2921\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2922\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2923\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2924\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2925\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2926\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2927\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2928\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2982\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2974\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2975\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2979\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2980\u001b[0m )\n\u001b[0;32m-> 2982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   2983\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2984\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2985\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2986\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2987\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2988\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2989\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2990\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2991\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2992\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2993\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2994\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2995\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2996\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2997\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2998\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2999\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3000\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3001\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m    577\u001b[0m         batched_input,\n\u001b[1;32m    578\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    579\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    580\u001b[0m         padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    581\u001b[0m         truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m    582\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    583\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    584\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    585\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    586\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    587\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    588\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    589\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    590\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    591\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    592\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    594\u001b[0m     )\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    505\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    506\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    507\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def preprocess_function(datum):\n",
    "    src, tgt = datum['text'].split(' <SEP> ')\n",
    "    return tokenizer(src, tgt, padding='max_length', max_length=512, truncation='longest_first')\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function)\n",
    "tokenized_val = val_dataset.map(preprocess_function)\n",
    "tokenized_test = test_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 'PV', 0: 'Garnett', 2: 'Katz', 3: 'McDuff', 1: 'Hogarth'}\n",
      "{'PV': 4, 'Garnett': 0, 'Katz': 2, 'McDuff': 3, 'Hogarth': 1}\n"
     ]
    }
   ],
   "source": [
    "label_list = translator_to_pars.keys()\n",
    "id_list = le.transform(list(label_list))\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for l, i in zip(label_list, id_list):\n",
    "    id2label[i] = l\n",
    "    label2id[l] = i\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# device (turn on GPU acceleration for faster execution)\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "if fine_tune:\n",
    "    # model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels = len(translator_to_pars.keys()))\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "epochs = 2\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mnk7klqb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df29408943ff489298df9de21cf9f121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prosperous-lantern-24</strong> at: <a href='https://wandb.ai/kkatsy/translator-classification-aligned/runs/mnk7klqb' target=\"_blank\">https://wandb.ai/kkatsy/translator-classification-aligned/runs/mnk7klqb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240222_230247-mnk7klqb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mnk7klqb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b657a5ee47646e3b81e64139252b0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112201211249662, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kkatsy/litMT/wandb/run-20240222_230418-wl371glh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kkatsy/translator-classification-aligned/runs/wl371glh' target=\"_blank\">chromatic-orchid-25</a></strong> to <a href='https://wandb.ai/kkatsy/translator-classification-aligned' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kkatsy/translator-classification-aligned' target=\"_blank\">https://wandb.ai/kkatsy/translator-classification-aligned</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kkatsy/translator-classification-aligned/runs/wl371glh' target=\"_blank\">https://wandb.ai/kkatsy/translator-classification-aligned/runs/wl371glh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "run = wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"translator-classification-aligned\",\n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"epochs\": epochs,\n",
    "        },\n",
    "    )\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"]=\"translator-classification-aligned\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"]=\"translator-classification-aligned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "if fine_tune:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/trunk/kkatsy/test\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"wandb\",\n",
    "        logging_dir=\"/home/kkatsy/litMT/logs\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_first_step=True\n",
    "\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "NCCL Error 2: unhandled system error (run with NCCL_DEBUG=INFO for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fine_tune:\n\u001b[0;32m----> 2\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2768\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2768\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2771\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2791\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2790\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2792\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:189\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplicate\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m replicate(module, device_ids, \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/replicate.py:110\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    108\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    109\u001b[0m param_indices \u001b[38;5;241m=\u001b[39m {param: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params)}\n\u001b[0;32m--> 110\u001b[0m param_copies \u001b[38;5;241m=\u001b[39m _broadcast_coalesced_reshape(params, devices, detach)\n\u001b[1;32m    112\u001b[0m buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mbuffers())\n\u001b[1;32m    113\u001b[0m buffers_rg: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/replicate.py:83\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m         tensor_copies \u001b[38;5;241m=\u001b[39m Broadcast\u001b[38;5;241m.\u001b[39mapply(devices, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [tensor_copies[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensors)]\n\u001b[1;32m     85\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tensor_copies), \u001b[38;5;28mlen\u001b[39m(tensors))]\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:23\u001b[0m, in \u001b[0;36mBroadcast.forward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m ctx\u001b[38;5;241m.\u001b[39mnum_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m     22\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_device()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mbroadcast_coalesced(inputs, ctx\u001b[38;5;241m.\u001b[39mtarget_gpus)\n\u001b[1;32m     24\u001b[0m non_differentiables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, input_requires_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ctx\u001b[38;5;241m.\u001b[39mneeds_input_grad[\u001b[38;5;241m1\u001b[39m:]):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/comm.py:57\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[1;32m     56\u001b[0m tensors \u001b[38;5;241m=\u001b[39m [_handle_complex(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_broadcast_coalesced(tensors, devices, buffer_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NCCL Error 2: unhandled system error (run with NCCL_DEBUG=INFO for details)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if fine_tune:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkatsy/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if fine_tune:\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce9fde6be4e4647b4648b5bc9d875aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁</td></tr><tr><td>eval/loss</td><td>▁▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁</td></tr><tr><td>train/global_step</td><td>▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.63863</td></tr><tr><td>eval/loss</td><td>0.89935</td></tr><tr><td>eval/runtime</td><td>11.7205</td></tr><tr><td>eval/samples_per_second</td><td>92.317</td></tr><tr><td>eval/steps_per_second</td><td>3.925</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>739</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.792</td></tr><tr><td>train/total_flos</td><td>4666400256599040.0</td></tr><tr><td>train/train_loss</td><td>0.79197</td></tr><tr><td>train/train_runtime</td><td>469.9671</td></tr><tr><td>train/train_samples_per_second</td><td>37.737</td></tr><tr><td>train/train_steps_per_second</td><td>1.572</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-dragon-21</strong> at: <a href='https://wandb.ai/kkatsy/translator-classification-aligned/runs/6c07jsut' target=\"_blank\">https://wandb.ai/kkatsy/translator-classification-aligned/runs/6c07jsut</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240222_205353-6c07jsut/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_tuned = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_tuned:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"/trunk/kkatsy/aligned_20epochs_holdout\")\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_tuned:\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_tuned:\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkatsy/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(tokenized_test, metric_key_prefix=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PV': 288, 'Garnett': 205, 'Katz': 229, 'McDuff': 194, 'Hogarth': 166}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translators = [id2label[l] for l in preds]\n",
    "\n",
    "pred_count = {}\n",
    "for i in label2id.keys():\n",
    "    count = translators.count(i)\n",
    "    pred_count[i] = count\n",
    "\n",
    "pred_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "courses = list(pred_count.keys())\n",
    "values = list(pred_count.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(courses, values, width = 0.4)\n",
    "plt.title(\"Predicted Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PV': 217, 'Garnett': 217, 'Katz': 216, 'McDuff': 216, 'Hogarth': 216}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_count = {}\n",
    "for i in id2label.keys():\n",
    "    count = test_labels.count(i)\n",
    "    true_count[id2label[i]] = count\n",
    "\n",
    "true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "courses = list(true_count.keys())\n",
    "values = list(true_count.values())\n",
    "  \n",
    "fig2 = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(courses, values, width = 0.4)\n",
    "plt.title(\"Actual Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6386321626617375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "acc = metrics.accuracy_score(labels, preds)\n",
    "print('Accuracy: ', acc)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(labels, preds, normalize='pred')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels=label2id)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idiom lookup: search russian and english datasets for idioms\n",
    "# english idiom dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
